{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 19:53:07.013396: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.12.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(740, 15, 20)\n",
      "(740, 15, 31)\n",
      "(740, 15, 20)\n",
      "(740, 2)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical as labelEncoding \n",
    "\n",
    "T = 15 # terminus_length\n",
    "\n",
    "X1 = np.load('bpf-740.npy')\n",
    "\n",
    "\n",
    "X1 = X1[:,0:T,:]\n",
    "\n",
    "\n",
    "Y  = [1 for _ in range(376)]\n",
    "Y += [0 for _ in range(364)]\n",
    "\n",
    "Y = labelEncoding(Y, dtype=int)\n",
    "\n",
    "print(X1.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(740, 15, 20)\n",
      "(15, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X1.shape)\n",
    "print(X1[0].shape)\n",
    "X1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're using TF-2.12.0.\n"
     ]
    }
   ],
   "source": [
    "# Deep Neural Networks:\n",
    "import tensorflow as tf; print('We\\'re using TF-{}.'.format(tf.__version__))\n",
    "# import keras; print('We\\'re using Keras-{}.'.format(keras.__version__))\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, Flatten, BatchNormalization,\n",
    "                                     Conv1D, Conv2D, MaxPooling1D, MaxPooling2D,\n",
    "                                     LSTM, GRU, Embedding, Bidirectional, Concatenate)\n",
    "from tensorflow.keras.regularizers import (l1, l2, l1_l2)\n",
    "from tensorflow.keras.optimizers import (RMSprop, Adam, SGD)\n",
    "from tensorflow.keras.models import (Sequential, Model)\n",
    "\n",
    "# Core:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Performance:\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, matthews_corrcoef, precision_score, roc_curve, auc)\n",
    "from sklearn.model_selection import (StratifiedKFold, KFold, train_test_split)\n",
    "\n",
    "#Utilities:\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical as labelEncoding   # Usages: Y = labelEncoding(Y, dtype=int)\n",
    "from tensorflow.keras.utils import plot_model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    ### Head-1:\n",
    "    input1 = Input(shape=X1[0].shape)\n",
    "\n",
    "    x = Conv1D(filters=10, kernel_size=4, padding='same', activation='relu', kernel_regularizer=l2(l=0.01))(input1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.80)(x)\n",
    "\n",
    "    x = Conv1D(filters=8, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(l=0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.70)(x)\n",
    "\n",
    "    head1 = Flatten()(x)\n",
    "\n",
    "    # merge\n",
    "    merge = Concatenate()([head1])\n",
    "\n",
    "    output = Dense(units=8, activation='relu', kernel_regularizer=l2(l=0.01))(merge)\n",
    "    output = BatchNormalization()(output)\n",
    "    output = Dropout(rate=0.70)(output)\n",
    "\n",
    "    output = Dense(units=2, activation='softmax')(output)\n",
    "\n",
    "    return Model(inputs=[input1], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/homayoun/anaconda3/envs/acp-proj/lib/python3.11/site-packages/Bio/SubsMat/__init__.py:126: BiopythonDeprecationWarning: Bio.SubsMat has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.substitution_matrices as a replacement, and contact the Biopython developers if you still need the Bio.SubsMat module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from Bio.Seq import Seq\n",
    "from Bio.SeqUtils import ProtParam\n",
    "from Bio.Align import substitution_matrices\n",
    "from Bio.SubsMat import MatrixInfo\n",
    "\n",
    "\n",
    "# Function to convert binary profile feature to amino acid sequence\n",
    "def bpf_to_sequence(binary_profile_feature):\n",
    "    amino_acids = \"ARNDCQEGHILKMFPSTWYVBZX\"\n",
    "    sequence = \"\"\n",
    "    for row in binary_profile_feature:\n",
    "        index = row.argmax()\n",
    "        sequence += amino_acids[index]\n",
    "    return sequence\n",
    "\n",
    "def seq_to_blosum(sequence):\n",
    "    blosum62 = MatrixInfo.blosum62\n",
    "    sequence = sequence.upper()\n",
    "    length = len(sequence)\n",
    "    blosum_matrix = []\n",
    "    for aa1 in sequence:\n",
    "        row = []\n",
    "        for aa2 in sequence:\n",
    "            if (aa1, aa2) in blosum62:\n",
    "                row.append(blosum62[(aa1, aa2)])\n",
    "            else:\n",
    "                row.append(blosum62[(aa2, aa1)])\n",
    "        blosum_matrix.append(row)\n",
    "    return blosum_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpf_to_sequence(binary_profile_feature):\n",
    "    amino_acids = \"ARNDCQEGHILKMFPSTWYVBZX\"\n",
    "    sequence = \"\"\n",
    "    for row in binary_profile_feature:\n",
    "        i = 0\n",
    "        for item in row:\n",
    "            i += 1\n",
    "            if item.numpy() == 1:\n",
    "                sequence += amino_acids[i]\n",
    "    return sequence\n",
    "\n",
    "def seq_to_blosum(sequence):\n",
    "    blosum62 = MatrixInfo.blosum62\n",
    "    sequence = sequence.upper()\n",
    "    length = len(sequence)\n",
    "    blosum_matrix = []\n",
    "    for aa1 in sequence:\n",
    "        row = []\n",
    "        for aa2 in sequence:\n",
    "            if (aa1, aa2) in blosum62:\n",
    "                row.append(blosum62[(aa1, aa2)])\n",
    "            else:\n",
    "                row.append(blosum62[(aa2, aa1)])\n",
    "        blosum_matrix.append(row[:20])\n",
    "    return blosum_matrix\n",
    "\n",
    "def bpf_to_blosum_layer(seqs_bpf):\n",
    "    seqs_blosum = []\n",
    "    for bpf in seqs_bpf:\n",
    "        str_seq = bpf_to_sequence(bpf)\n",
    "        seqs_blosum.append(seq_to_blosum(str_seq))\n",
    "\n",
    "    print(seqs_blosum)\n",
    "    return seqs_blosum\n",
    "\n",
    "def build_generator(latent_dim, output_shape):\n",
    "    model = Sequential([\n",
    "        layers.Input(shape=(latent_dim,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(output_shape[0] * output_shape[1], activation='sigmoid'),\n",
    "        layers.Reshape(output_shape),\n",
    "        layers.Lambda(lambda x: tf.one_hot(tf.argmax(x, axis=-1), depth=output_shape[1])),\n",
    "        layers.Lambda(lambda x: tf.cast(x, dtype=tf.int32)),\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[6, -1, 0, 0, 6, 3, -3, -3, 3, 0, -3, -2, 6, 6, 1, -3, 0, -2, 1, -3], [-1, 8, -2, -3, -1, 2, 1, -2, 2, -3, 0, -3, -1, -1, -2, -2, -2, -3, -2, -1], [0, -2, 5, 2, 0, -1, -2, -3, -1, 1, -2, -1, 0, 0, -1, -3, 5, -1, -1, -1], [0, -3, 2, 4, 0, -1, -3, -4, -1, 2, -3, -1, 0, 0, -2, -4, 2, -1, -2, -2], [6, -1, 0, 0, 6, 3, -3, -3, 3, 0, -3, -2, 6, 6, 1, -3, 0, -2, 1, -3], [3, 2, -1, -1, 3, 7, -2, -3, 7, -1, -2, -2, 3, 3, 2, -3, -1, -2, 2, -2], [-3, 1, -2, -3, -3, -2, 6, 0, -2, -3, 0, -3, -3, -3, -4, 0, -2, -3, -4, 0], [-3, -2, -3, -4, -3, -3, 0, 6, -3, -4, -2, -3, -3, -3, -2, 6, -3, -3, -2, -2], [3, 2, -1, -1, 3, 7, -2, -3, 7, -1, -2, -2, 3, 3, 2, -3, -1, -2, 2, -2], [0, -3, 1, 2, 0, -1, -3, -4, -1, 4, -3, -1, 0, 0, -3, -4, 1, -1, -3, -3], [-3, 0, -2, -3, -3, -2, 0, -2, -2, -3, 5, -4, -3, -3, -3, -2, -2, -4, -3, 1], [-2, -3, -1, -1, -2, -2, -3, -3, -2, -1, -4, 9, -2, -2, -2, -3, -1, 9, -2, -3], [6, -1, 0, 0, 6, 3, -3, -3, 3, 0, -3, -2, 6, 6, 1, -3, 0, -2, 1, -3], [6, -1, 0, 0, 6, 3, -3, -3, 3, 0, -3, -2, 6, 6, 1, -3, 0, -2, 1, -3], [1, -2, -1, -2, 1, 2, -4, -2, 2, -3, -3, -2, 1, 1, 11, -2, -1, -2, 11, -3], [-3, -2, -3, -4, -3, -3, 0, 6, -3, -4, -2, -3, -3, -3, -2, 6, -3, -3, -2, -2], [0, -2, 5, 2, 0, -1, -2, -3, -1, 1, -2, -1, 0, 0, -1, -3, 5, -1, -1, -1], [-2, -3, -1, -1, -2, -2, -3, -3, -2, -1, -4, 9, -2, -2, -2, -3, -1, 9, -2, -3], [1, -2, -1, -2, 1, 2, -4, -2, 2, -3, -3, -2, 1, 1, 11, -2, -1, -2, 11, -3], [-3, -1, -1, -2, -3, -2, 0, -2, -2, -3, 1, -3, -3, -3, -3, -2, -1, -3, -3, 5], [-1, 8, -2, -3, -1, 2, 1, -2, 2, -3, 0, -3, -1, -1, -2, -2, -2, -3, -2, -1], [0, -3, 1, 2, 0, -1, -3, -4, -1, 4, -3, -1, 0, 0, -3, -4, 1, -1, -3, -3], [-2, -3, -1, -1, -2, -2, -3, -3, -2, -1, -4, 9, -2, -2, -2, -3, -1, 9, -2, -3], [-2, -1, -1, -2, -2, -2, 1, 0, -2, -2, 0, -1, -2, -2, -3, 0, -1, -1, -3, 0], [-2, -3, -1, -1, -2, -2, -3, -3, -2, -1, -4, 9, -2, -2, -2, -3, -1, 9, -2, -3]]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'lambda_73' (type Lambda).\n\n{{function_node __wrapped__Pack_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Shapes of all inputs must match: values[0].shape = [1,25,20] != values[1].shape = [25,20] [Op:Pack] name: stack\n\nCall arguments received by layer 'lambda_73' (type Lambda):\n  • inputs=tf.Tensor(shape=(1, 25, 20), dtype=int32)\n  • mask=None\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m generator \u001b[38;5;241m=\u001b[39m build_generator(latent_dim, output_shape)\n\u001b[1;32m      8\u001b[0m noise \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal([\u001b[38;5;241m1\u001b[39m, latent_dim])\n\u001b[0;32m---> 10\u001b[0m generated_matrix \u001b[38;5;241m=\u001b[39m generator(noise, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m generated_matrix\n",
      "File \u001b[0;32m~/anaconda3/envs/acp-proj/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[55], line 49\u001b[0m, in \u001b[0;36mbuild_generator.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_generator\u001b[39m(latent_dim, output_shape):\n\u001b[1;32m     37\u001b[0m     model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[1;32m     38\u001b[0m         layers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(latent_dim,)),\n\u001b[1;32m     39\u001b[0m         layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m         layers\u001b[38;5;241m.\u001b[39mLambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: tf\u001b[38;5;241m.\u001b[39mcast(x, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32)),\n\u001b[1;32m     45\u001b[0m     ])\n\u001b[1;32m     47\u001b[0m     model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[1;32m     48\u001b[0m         model,\n\u001b[0;32m---> 49\u001b[0m         layers\u001b[38;5;241m.\u001b[39mLambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: tf\u001b[38;5;241m.\u001b[39mstack([\n\u001b[1;32m     50\u001b[0m             x, \n\u001b[1;32m     51\u001b[0m             tf\u001b[38;5;241m.\u001b[39mpy_function(bpf_to_blosum_layer, [x], tf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m     52\u001b[0m         ], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)),\n\u001b[1;32m     53\u001b[0m     ])\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'lambda_73' (type Lambda).\n\n{{function_node __wrapped__Pack_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Shapes of all inputs must match: values[0].shape = [1,25,20] != values[1].shape = [25,20] [Op:Pack] name: stack\n\nCall arguments received by layer 'lambda_73' (type Lambda):\n  • inputs=tf.Tensor(shape=(1, 25, 20), dtype=int32)\n  • mask=None\n  • training=False"
     ]
    }
   ],
   "source": [
    "# Define the dimensions\n",
    "latent_dim = 100  # Dimensionality of the latent space\n",
    "batch_size = 1\n",
    "output_shape = (25, 20)  # Desired output shape of the matrix\n",
    "\n",
    "generator = build_generator(latent_dim, output_shape)\n",
    "\n",
    "noise = tf.random.normal([1, latent_dim])\n",
    "\n",
    "generated_matrix = generator(noise, training=False)\n",
    "\n",
    "generated_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_discriminator()\n",
    "discriminator.load_weights('./acp_mhcnn_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 414ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.22306105, 0.7769389 ],\n",
       "       [0.2842041 , 0.7157959 ],\n",
       "       [0.31516826, 0.68483174],\n",
       "       [0.29036   , 0.70963997],\n",
       "       [0.30475914, 0.69524086]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = discriminator.predict([X1[:5,:,:]])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[ 0  0  0 ...  0  0  0]\n",
      "   [ 0  0  0 ...  0  0  0]\n",
      "   [ 0  0  0 ...  0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0 ...  0  0  0]\n",
      "   [ 0  0  0 ...  0  0  0]\n",
      "   [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0 ...  0  0  0]\n",
      "   [ 0  0  0 ...  0  0  0]\n",
      "   [ 0  0  0 ...  0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0 ...  0  0  0]\n",
      "   [ 0  0  0 ...  0  0  1]\n",
      "   [ 1  0  0 ...  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0 ...  0  0  0]\n",
      "   [ 0  0  0 ...  0  0  0]\n",
      "   [ 0  0  0 ...  0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0 ...  0  0  0]\n",
      "   [ 0  0  0 ...  0  0  0]\n",
      "   [ 0  0  0 ...  0  1  0]]\n",
      "\n",
      "  [[ 0  0  0 ...  0  0  0]\n",
      "   [ 0  0  0 ...  0  0  0]\n",
      "   [ 0  0  0 ...  1  0  0]\n",
      "   ...\n",
      "   [ 0  0  0 ...  0  0  0]\n",
      "   [ 0  0  0 ...  0  0  0]\n",
      "   [ 1  0  0 ...  0  0  0]]\n",
      "\n",
      "  [[ 0  0  1 ...  0  0  0]\n",
      "   [ 0  0  0 ...  0  0  0]\n",
      "   [ 0  0  0 ...  1  0  0]\n",
      "   ...\n",
      "   [ 0  1  0 ...  0  0  0]\n",
      "   [ 0  0  0 ...  0  0  0]\n",
      "   [ 0  0  0 ...  0  0  0]]]\n",
      "\n",
      "\n",
      " [[[ 5 -1 -1 ... -2 -1 -2]\n",
      "   [-1  5 -1 ...  0 -2 -1]\n",
      "   [-1 -1  5 ...  1 -2  2]\n",
      "   ...\n",
      "   [-1 -2 -2 ... -3  8 -3]\n",
      "   [-3 -1  1 ...  3 -3  2]\n",
      "   [ 1 -1  0 ... -2  0 -2]]\n",
      "\n",
      "  [[ 5 -3 -2 ...  5 -3  1]\n",
      "   [-3  6  0 ... -3  0 -3]\n",
      "   [-2  0  4 ... -2  2 -2]\n",
      "   ...\n",
      "   [-1 -1 -3 ... -1 -3  0]\n",
      "   [-2 -1  1 ... -2  3 -2]\n",
      "   [-1 -2 -1 ... -1 -1 -1]]\n",
      "\n",
      "  [[ 6 -4 -3 ...  3 -3  0]\n",
      "   [-4  7 -2 ... -3 -2 -3]\n",
      "   [-3 -2  6 ... -3  6 -4]\n",
      "   ...\n",
      "   [ 0 -2 -3 ... -1 -3  2]\n",
      "   [-3 -1 -2 ... -1 -2 -2]\n",
      "   [ 3 -3 -3 ...  7 -3 -1]]\n",
      "\n",
      "  [[ 4 -3 -3 ... -3 -3 -3]\n",
      "   [-3  7 -4 ... -1 -4 -1]\n",
      "   [-3 -4 11 ... -3 11 -3]\n",
      "   ...\n",
      "   [-3 -2 -2 ... -1 -2  0]\n",
      "   [ 4 -3 -3 ... -3 -3 -3]\n",
      "   [-1 -1 -3 ... -1 -3 -1]]\n",
      "\n",
      "  [[ 6 -3 -4 ...  1 -4 -3]\n",
      "   [-3  4 -3 ... -3 -3  4]\n",
      "   [-4 -3 11 ... -2 11 -3]\n",
      "   ...\n",
      "   [ 0 -3 -3 ...  0 -3 -3]\n",
      "   [-3  4 -3 ... -3 -3  4]\n",
      "   [-2 -3 -4 ... -2 -4 -3]]]], shape=(2, 5, 25, 20), dtype=int32)\n",
      "1/1 [==============================] - 0s 306ms/step\n",
      "[[0.42349637 0.5765037 ]\n",
      " [0.17079028 0.8292097 ]\n",
      " [0.5574622  0.44253775]\n",
      " [0.35372564 0.6462744 ]\n",
      " [0.38007596 0.61992407]]\n"
     ]
    }
   ],
   "source": [
    "noise = tf.random.normal([5, 100])\n",
    "fake_seqs = generator(noise, training=False)\n",
    "\n",
    "# inputs_bpf = []\n",
    "# inputs_blosum = []\n",
    "# for seq in fake_seqs:\n",
    "#     bpf, blosum = convert_input(seq)\n",
    "#     inputs_bpf.append(bpf[:15])\n",
    "#     inputs_blosum.append(blosum[:15,:20])\n",
    "\n",
    "# inputs_bpf = np.array(inputs_bpf)\n",
    "# inputs_blosum = np.array(inputs_blosum)\n",
    "# fake_seqs = fake_seqs[:,:15,:]\n",
    "\n",
    "\n",
    "print(fake_seqs)\n",
    "\n",
    "# print(X3[:5,:,:].shape)\n",
    "# print(X3[0])\n",
    "# print('---------------')\n",
    "# print(inputs_blosum.shape)\n",
    "# print(inputs_blosum[0])\n",
    "\n",
    "prediction = discriminator.predict([fake_seqs[:,:15,:]])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(disc_output):\n",
    "    # print(disc_output.shape)\n",
    "    desired_output = np.zeros_like(disc_output)\n",
    "    desired_output[:, 1] = 1\n",
    "    return cross_entropy(desired_output, disc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.6327607, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/acp-proj/lib/python3.10/site-packages/keras/backend.py:5676: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, 100])\n",
    "fake_seqs_sym = generator(noise, training=True)\n",
    "\n",
    "fake_seqs_sym = fake_seqs_sym[:,:15,:]\n",
    "fake_output = discriminator([fake_seqs[:,:15,:]])\n",
    "gen_loss = generator_loss(fake_output)\n",
    "print(gen_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step():\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        fake_seqs_sym = generator(noise, training=True)\n",
    "\n",
    "        fake_seqs_bpf = fake_seqs_sym[:,:15,:]\n",
    "\n",
    "        print('test')\n",
    "        fake_output = discriminator([fake_seqs])\n",
    "        print(fake_output)\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        print('loss', gen_loss)\n",
    "            \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    for gradient in gradients_of_generator:\n",
    "        print(\"Gradient:\", gradient)\n",
    "    \n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "def train(dataset = None, epochs = 50):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for batch in range(100):\n",
    "            # train_step(batch[0], batch[1])\n",
    "            train_step()\n",
    "\n",
    "        # Generate and save some examples of generated sequences\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            noise = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "            generated_sequences = generator(noise, training=False)\n",
    "            print(\"Examples of generated sequences at epoch\", epoch + 1)\n",
    "            for i in range(num_examples_to_generate):\n",
    "                print(bpf_to_sequence(generated_sequences[i].numpy()))\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_384410/1021801238.py\", line 6, in train_step  *\n        fake_seqs_sym = generator(noise, training=True)\n    File \"/home/user/anaconda3/envs/acp-proj/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/ipykernel_384410/438853299.py\", line 46, in <lambda>\n        layers.Lambda(lambda x: tf.stack([x, bpf_to_blosum_layer(x)], axis=0))\n    File \"/tmp/ipykernel_384410/438853299.py\", line 27, in bpf_to_blosum_layer\n        for bpf in seqs_bpf:\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling layer 'lambda_2' (type Lambda).\n    \n    Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n    \n    Call arguments received by layer 'lambda_2' (type Lambda):\n      • inputs=tf.Tensor(shape=(32, 25, 20), dtype=int32)\n      • mask=None\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Combine datasets X1 and X3\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# combined_dataset = tf.data.Dataset.from_tensor_slices((X1, X3)).shuffle(len(X1)).batch(BATCH_SIZE)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model for 50 epochs\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 48\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m     44\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# train_step(batch[0], batch[1])\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Generate and save some examples of generated sequences\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/acp-proj/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/acp-proj/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1269\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m   1270\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_384410/1021801238.py\", line 6, in train_step  *\n        fake_seqs_sym = generator(noise, training=True)\n    File \"/home/user/anaconda3/envs/acp-proj/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/ipykernel_384410/438853299.py\", line 46, in <lambda>\n        layers.Lambda(lambda x: tf.stack([x, bpf_to_blosum_layer(x)], axis=0))\n    File \"/tmp/ipykernel_384410/438853299.py\", line 27, in bpf_to_blosum_layer\n        for bpf in seqs_bpf:\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling layer 'lambda_2' (type Lambda).\n    \n    Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n    \n    Call arguments received by layer 'lambda_2' (type Lambda):\n      • inputs=tf.Tensor(shape=(32, 25, 20), dtype=int32)\n      • mask=None\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "# Combine datasets X1 and X3\n",
    "# combined_dataset = tf.data.Dataset.from_tensor_slices((X1, X3)).shuffle(len(X1)).batch(BATCH_SIZE)\n",
    "\n",
    "# Train the model for 50 epochs\n",
    "train(epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
