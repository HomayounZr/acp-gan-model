{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 18:18:54.012984: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(376, 15, 20)\n",
      "(376, 2)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical as labelEncoding \n",
    "\n",
    "T = 15 # terminus_length\n",
    "\n",
    "X1 = np.load('bpf-740.npy')\n",
    "\n",
    "\n",
    "X1 = X1[:376,:T,:]\n",
    "\n",
    "\n",
    "Y  = [1 for _ in range(376)]\n",
    "\n",
    "Y = labelEncoding(Y, dtype=int)\n",
    "\n",
    "print(X1.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(376, 15, 20)\n",
      "(15, 20)\n",
      "[[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 18:19:02.135014: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-08 18:19:02.138701: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "print(X1.shape)\n",
    "print(X1[0].shape)\n",
    "print(X1[0])\n",
    "\n",
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(X1).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're using TF-2.11.0.\n"
     ]
    }
   ],
   "source": [
    "# Deep Neural Networks:\n",
    "import tensorflow as tf; print('We\\'re using TF-{}.'.format(tf.__version__))\n",
    "# import keras; print('We\\'re using Keras-{}.'.format(keras.__version__))\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, Flatten, BatchNormalization,\n",
    "                                     Conv1D, Conv2D, MaxPooling1D, MaxPooling2D,\n",
    "                                     LSTM, GRU, Embedding, Bidirectional, Concatenate)\n",
    "from tensorflow.keras.regularizers import (l1, l2, l1_l2)\n",
    "from tensorflow.keras.optimizers import (RMSprop, Adam, SGD)\n",
    "from tensorflow.keras.models import (Sequential, Model)\n",
    "\n",
    "# Core:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Performance:\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, matthews_corrcoef, precision_score, roc_curve, auc)\n",
    "from sklearn.model_selection import (StratifiedKFold, KFold, train_test_split)\n",
    "\n",
    "#Utilities:\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical as labelEncoding   # Usages: Y = labelEncoding(Y, dtype=int)\n",
    "from tensorflow.keras.utils import plot_model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    ### Head-1:\n",
    "    input1 = Input(shape=X1[0].shape)\n",
    "\n",
    "    x = Conv1D(filters=10, kernel_size=4, padding='same', activation='relu', kernel_regularizer=l2(l=0.01))(input1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.80)(x)\n",
    "\n",
    "    x = Conv1D(filters=8, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(l=0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.70)(x)\n",
    "\n",
    "    head1 = Flatten()(x)\n",
    "\n",
    "    # merge\n",
    "    merge = Concatenate()([head1])\n",
    "\n",
    "    output = Dense(units=8, activation='relu', kernel_regularizer=l2(l=0.01))(merge)\n",
    "    output = BatchNormalization()(output)\n",
    "    output = Dropout(rate=0.70)(output)\n",
    "\n",
    "    output = Dense(units=2, activation='softmax')(output)\n",
    "\n",
    "    return Model(inputs=[input1], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/homayoun/anaconda3/envs/acp-proj/lib/python3.10/site-packages/Bio/SubsMat/__init__.py:126: BiopythonDeprecationWarning: Bio.SubsMat has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.substitution_matrices as a replacement, and contact the Biopython developers if you still need the Bio.SubsMat module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from Bio.Seq import Seq\n",
    "from Bio.SeqUtils import ProtParam\n",
    "from Bio.Align import substitution_matrices\n",
    "from Bio.SubsMat import MatrixInfo\n",
    "\n",
    "\n",
    "# Function to convert binary profile feature to amino acid sequence\n",
    "def bpf_to_sequence(binary_profile_feature):\n",
    "    amino_acids = \"ARNDCQEGHILKMFPSTWYVBZX\"\n",
    "    sequence = \"\"\n",
    "    for row in binary_profile_feature:\n",
    "        index = row.argmax()\n",
    "        sequence += amino_acids[index]\n",
    "    return sequence\n",
    "\n",
    "def seq_to_blosum(sequence):\n",
    "    blosum62 = MatrixInfo.blosum62\n",
    "    sequence = sequence.upper()\n",
    "    length = len(sequence)\n",
    "    blosum_matrix = []\n",
    "    for aa1 in sequence:\n",
    "        row = []\n",
    "        for aa2 in sequence:\n",
    "            if (aa1, aa2) in blosum62:\n",
    "                row.append(blosum62[(aa1, aa2)])\n",
    "            else:\n",
    "                row.append(blosum62[(aa2, aa1)])\n",
    "        blosum_matrix.append(row)\n",
    "    return blosum_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpf_to_sequence(binary_profile_feature):\n",
    "    amino_acids = \"ARNDCQEGHILKMFPSTWYVBZX\"\n",
    "    sequence = \"\"\n",
    "    for row in binary_profile_feature:\n",
    "        i = 0\n",
    "        for item in row:\n",
    "            i += 1\n",
    "            if item.numpy() == 1:\n",
    "                sequence += amino_acids[i]\n",
    "    return sequence\n",
    "\n",
    "def seq_to_blosum(sequence):\n",
    "    blosum62 = MatrixInfo.blosum62\n",
    "    sequence = sequence.upper()\n",
    "    length = len(sequence)\n",
    "    blosum_matrix = []\n",
    "    for aa1 in sequence:\n",
    "        row = []\n",
    "        for aa2 in sequence:\n",
    "            if (aa1, aa2) in blosum62:\n",
    "                row.append(blosum62[(aa1, aa2)])\n",
    "            else:\n",
    "                row.append(blosum62[(aa2, aa1)])\n",
    "        blosum_matrix.append(row[:20])\n",
    "    return blosum_matrix\n",
    "\n",
    "def bpf_to_blosum_layer(seqs_bpf):\n",
    "    seqs_blosum = []\n",
    "    for bpf in seqs_bpf:\n",
    "        str_seq = bpf_to_sequence(bpf)\n",
    "        seqs_blosum.append(seq_to_blosum(str_seq))\n",
    "\n",
    "    print(seqs_blosum)\n",
    "    return seqs_blosum\n",
    "\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Dense(512))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Dense(25 * 20, activation='sigmoid'))\n",
    "    model.add(layers.Reshape((25, 20)))\n",
    "    return model\n",
    "    \n",
    "\n",
    "    # model = Sequential([\n",
    "    #     layers.Input(shape=(latent_dim,)),\n",
    "    #     layers.Dense(128, activation='relu'),\n",
    "    #     layers.Dense(256, activation='relu'),\n",
    "    #     layers.Dense(output_shape[0] * output_shape[1], activation='sigmoid'),\n",
    "    #     layers.Reshape(output_shape),\n",
    "    #     layers.Lambda(lambda x: tf.one_hot(tf.argmax(x, axis=-1), depth=output_shape[1])),\n",
    "    #     layers.Lambda(lambda x: tf.cast(x, dtype=tf.int32)),\n",
    "    # ])\n",
    "\n",
    "    # return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 25, 20), dtype=float32, numpy=\n",
       "array([[[0.59916043, 0.53571814, 0.40263578, 0.61033857, 0.5403845 ,\n",
       "         0.5831024 , 0.48558527, 0.4782551 , 0.5556825 , 0.48901847,\n",
       "         0.5192514 , 0.64240897, 0.4109416 , 0.47700214, 0.5168907 ,\n",
       "         0.49550176, 0.64228076, 0.44921458, 0.6389669 , 0.5415116 ],\n",
       "        [0.42288995, 0.5490418 , 0.59424645, 0.47959942, 0.57634515,\n",
       "         0.6427886 , 0.4730355 , 0.41809404, 0.5134199 , 0.50252575,\n",
       "         0.42812222, 0.44326895, 0.5549914 , 0.5256387 , 0.58247584,\n",
       "         0.48176795, 0.47893462, 0.51944   , 0.56293356, 0.3404871 ],\n",
       "        [0.41126373, 0.4184705 , 0.5213183 , 0.57456857, 0.59182966,\n",
       "         0.55503994, 0.5803567 , 0.4063949 , 0.5829616 , 0.4649232 ,\n",
       "         0.5246829 , 0.45952612, 0.57522774, 0.49405873, 0.52228445,\n",
       "         0.5045324 , 0.4035373 , 0.5319075 , 0.55955607, 0.55667794],\n",
       "        [0.49701828, 0.52970284, 0.49671882, 0.46807227, 0.5598332 ,\n",
       "         0.5442624 , 0.4863615 , 0.6514802 , 0.5729724 , 0.48735183,\n",
       "         0.42169198, 0.52982104, 0.5635201 , 0.4691009 , 0.45354283,\n",
       "         0.48404983, 0.48718148, 0.5300746 , 0.4890982 , 0.4911348 ],\n",
       "        [0.47143248, 0.503347  , 0.41648683, 0.4303775 , 0.43808487,\n",
       "         0.476695  , 0.5171317 , 0.6141594 , 0.4914135 , 0.41585425,\n",
       "         0.4140074 , 0.38071212, 0.5544012 , 0.5461634 , 0.50395256,\n",
       "         0.60073304, 0.42636704, 0.5553147 , 0.528898  , 0.49624312],\n",
       "        [0.49399963, 0.40923285, 0.5392884 , 0.59360474, 0.4426865 ,\n",
       "         0.4257957 , 0.6498237 , 0.5613341 , 0.52126074, 0.46275133,\n",
       "         0.5042878 , 0.28917867, 0.4178133 , 0.52071744, 0.48304218,\n",
       "         0.51674354, 0.5396016 , 0.51552856, 0.44192594, 0.64004064],\n",
       "        [0.53692186, 0.42822918, 0.49483943, 0.6021025 , 0.59363675,\n",
       "         0.5280929 , 0.65192467, 0.73313314, 0.5777113 , 0.35831407,\n",
       "         0.3732673 , 0.53912836, 0.47466588, 0.48916507, 0.5399711 ,\n",
       "         0.5209188 , 0.6141184 , 0.60440797, 0.6963372 , 0.6479934 ],\n",
       "        [0.48807728, 0.4947078 , 0.45104238, 0.5693546 , 0.461454  ,\n",
       "         0.48438233, 0.3949673 , 0.4529448 , 0.5639488 , 0.43126285,\n",
       "         0.4721248 , 0.68098795, 0.42564306, 0.59449327, 0.4918585 ,\n",
       "         0.46058244, 0.5707351 , 0.6238829 , 0.5207493 , 0.45529875],\n",
       "        [0.51379627, 0.5714828 , 0.3746565 , 0.4440326 , 0.4963492 ,\n",
       "         0.5602951 , 0.41665098, 0.56396824, 0.47155404, 0.511339  ,\n",
       "         0.54752904, 0.5454734 , 0.44584173, 0.35402894, 0.49880913,\n",
       "         0.46875474, 0.4072393 , 0.50489736, 0.5255664 , 0.46795475],\n",
       "        [0.49297547, 0.37639716, 0.4184447 , 0.47756967, 0.44151196,\n",
       "         0.53749275, 0.47313538, 0.41398704, 0.5397334 , 0.49472326,\n",
       "         0.49300107, 0.63109976, 0.5634409 , 0.5854998 , 0.58996445,\n",
       "         0.618461  , 0.5644861 , 0.46664324, 0.42849353, 0.42288178],\n",
       "        [0.5139871 , 0.5490647 , 0.5776516 , 0.5752176 , 0.503838  ,\n",
       "         0.47823718, 0.43540904, 0.53690904, 0.45667094, 0.4653409 ,\n",
       "         0.6166662 , 0.5550706 , 0.5474941 , 0.45845678, 0.35374483,\n",
       "         0.53510404, 0.61052334, 0.4586139 , 0.46970412, 0.41004702],\n",
       "        [0.5335808 , 0.5901008 , 0.6102945 , 0.57730955, 0.4776014 ,\n",
       "         0.584099  , 0.42488068, 0.70399475, 0.54584557, 0.47960538,\n",
       "         0.61547333, 0.5454663 , 0.34437224, 0.49136108, 0.46874493,\n",
       "         0.43464875, 0.4050717 , 0.57816505, 0.6185486 , 0.5654044 ],\n",
       "        [0.43600768, 0.45747834, 0.5555432 , 0.46714342, 0.44501328,\n",
       "         0.3598682 , 0.6143474 , 0.5823065 , 0.5394272 , 0.4779363 ,\n",
       "         0.5232128 , 0.45949897, 0.48682848, 0.5723552 , 0.5925222 ,\n",
       "         0.51259977, 0.5022008 , 0.45437756, 0.41582927, 0.30173808],\n",
       "        [0.61217815, 0.5167393 , 0.46492824, 0.4272272 , 0.45478448,\n",
       "         0.477553  , 0.51712936, 0.56156325, 0.6133154 , 0.54368794,\n",
       "         0.4633823 , 0.30382112, 0.5450018 , 0.45643395, 0.5841688 ,\n",
       "         0.30835268, 0.54486287, 0.54544586, 0.5318086 , 0.40562627],\n",
       "        [0.49345413, 0.6266045 , 0.58597517, 0.5250077 , 0.42174247,\n",
       "         0.5195825 , 0.51808995, 0.5638164 , 0.39650056, 0.56523   ,\n",
       "         0.49207413, 0.48099312, 0.4958305 , 0.47185066, 0.32371256,\n",
       "         0.4440782 , 0.5058478 , 0.5651674 , 0.44072393, 0.5225112 ],\n",
       "        [0.5549799 , 0.4027164 , 0.44815853, 0.41179377, 0.5197092 ,\n",
       "         0.4660639 , 0.6196415 , 0.5993446 , 0.5939619 , 0.5138275 ,\n",
       "         0.4964127 , 0.43813473, 0.41119725, 0.5031914 , 0.48177126,\n",
       "         0.4610513 , 0.60945565, 0.36651573, 0.61554027, 0.4947969 ],\n",
       "        [0.46549377, 0.34281987, 0.6399211 , 0.52018964, 0.45776227,\n",
       "         0.48364848, 0.5436932 , 0.44637877, 0.507159  , 0.3808758 ,\n",
       "         0.4678633 , 0.42230684, 0.3911509 , 0.5176097 , 0.52039695,\n",
       "         0.515857  , 0.5394505 , 0.487251  , 0.42640722, 0.40332687],\n",
       "        [0.48018864, 0.52838355, 0.48164114, 0.49833488, 0.4653509 ,\n",
       "         0.3844834 , 0.31705007, 0.5076544 , 0.41576198, 0.4202687 ,\n",
       "         0.44344115, 0.47006154, 0.3372092 , 0.4808041 , 0.47141516,\n",
       "         0.45340288, 0.5801552 , 0.4759398 , 0.525147  , 0.5271722 ],\n",
       "        [0.47620288, 0.4858278 , 0.4882489 , 0.52291507, 0.5494232 ,\n",
       "         0.43119332, 0.46720156, 0.48975515, 0.58397067, 0.45498836,\n",
       "         0.5611383 , 0.54905653, 0.4491172 , 0.4350008 , 0.4438947 ,\n",
       "         0.60777366, 0.42529765, 0.6887396 , 0.5195929 , 0.4582508 ],\n",
       "        [0.48283967, 0.45314974, 0.30719158, 0.6444041 , 0.30114225,\n",
       "         0.3687365 , 0.5287524 , 0.46745956, 0.5081079 , 0.4532605 ,\n",
       "         0.6928191 , 0.6621508 , 0.46143812, 0.35577157, 0.47508305,\n",
       "         0.48908675, 0.5915075 , 0.475407  , 0.54199487, 0.4526547 ],\n",
       "        [0.46483183, 0.41738406, 0.4269173 , 0.5608634 , 0.5248616 ,\n",
       "         0.6352318 , 0.4898417 , 0.6185946 , 0.6092055 , 0.5764918 ,\n",
       "         0.55821013, 0.37653998, 0.41848174, 0.3889043 , 0.46220234,\n",
       "         0.42829138, 0.4955534 , 0.62440664, 0.5934475 , 0.59242594],\n",
       "        [0.44974524, 0.56495595, 0.494688  , 0.44340095, 0.36719033,\n",
       "         0.5564238 , 0.42915767, 0.45263872, 0.573033  , 0.6298892 ,\n",
       "         0.5606563 , 0.6789281 , 0.49323428, 0.36508316, 0.48196816,\n",
       "         0.6424249 , 0.5974535 , 0.31042418, 0.5411445 , 0.5580202 ],\n",
       "        [0.48212194, 0.52377224, 0.2905166 , 0.5579637 , 0.6290041 ,\n",
       "         0.6631787 , 0.46285066, 0.38900274, 0.48090822, 0.4947443 ,\n",
       "         0.46147913, 0.54381895, 0.42805666, 0.41915956, 0.4328364 ,\n",
       "         0.33911332, 0.44688874, 0.45391047, 0.5079363 , 0.47111803],\n",
       "        [0.39927813, 0.53998005, 0.5650597 , 0.5005157 , 0.43439326,\n",
       "         0.6078724 , 0.3042463 , 0.50108314, 0.5229841 , 0.4813098 ,\n",
       "         0.48518616, 0.48108178, 0.51897407, 0.48624793, 0.41586715,\n",
       "         0.43899232, 0.44062468, 0.4604363 , 0.498465  , 0.5362841 ],\n",
       "        [0.36224633, 0.45743194, 0.52497476, 0.6585588 , 0.46596923,\n",
       "         0.48158327, 0.34373748, 0.46963573, 0.5489708 , 0.47793478,\n",
       "         0.59051627, 0.6272264 , 0.6422179 , 0.5566056 , 0.47792953,\n",
       "         0.50081044, 0.5973143 , 0.41289815, 0.53139764, 0.5690447 ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the dimensions\n",
    "latent_dim = 100  # Dimensionality of the latent space\n",
    "batch_size = 1\n",
    "output_shape = (25, 20)  # Desired output shape of the matrix\n",
    "\n",
    "generator = build_generator()\n",
    "\n",
    "noise = tf.random.normal([batch_size, latent_dim])\n",
    "\n",
    "generated_matrix = generator(noise, training=False)\n",
    "\n",
    "generated_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_discriminator()\n",
    "discriminator.load_weights('./acp_mhcnn_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 111ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.30780783, 0.6921922 ],\n",
       "       [0.17504199, 0.824958  ],\n",
       "       [0.31832418, 0.68167585],\n",
       "       [0.18360284, 0.81639713],\n",
       "       [0.19149974, 0.80850023]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = discriminator.predict([X1[:5,:,:]])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "[[9.9999988e-01 8.5324515e-08]\n",
      " [9.9999964e-01 3.3498407e-07]\n",
      " [9.9999928e-01 6.6238709e-07]\n",
      " [9.9999940e-01 6.4296410e-07]\n",
      " [9.9999964e-01 3.1736076e-07]]\n"
     ]
    }
   ],
   "source": [
    "noise = tf.random.normal([5, 100])\n",
    "fake_seqs = generator(noise, training=False)\n",
    "\n",
    "prediction = discriminator.predict([fake_seqs[:,:15,:]])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = tf.keras.losses.BinaryCrossentropy()(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = tf.keras.losses.BinaryCrossentropy()(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return tf.keras.losses.BinaryCrossentropy()(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "# def generator_loss(disc_output):\n",
    "#     batch_size = tf.shape(disc_output)[0]\n",
    "#     num_classes = tf.shape(disc_output)[1]\n",
    "#     desired_output = tf.concat([tf.zeros((batch_size, 1)), tf.ones((batch_size, num_classes - 1))], axis=1)\n",
    "    \n",
    "#     # Define binary cross entropy loss\n",
    "#     bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    \n",
    "#     # Calculate loss\n",
    "#     loss = bce(desired_output, disc_output)\n",
    "    \n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "[[9.9999988e-01 1.1416962e-07]\n",
      " [9.9999952e-01 4.5712932e-07]\n",
      " [9.9999952e-01 4.4334897e-07]\n",
      " [9.9999928e-01 6.6139603e-07]\n",
      " [9.9999893e-01 1.0680716e-06]]\n",
      "[[0.30780783 0.6921922 ]\n",
      " [0.17504199 0.824958  ]\n",
      " [0.31832418 0.68167585]\n",
      " [0.18360284 0.81639713]\n",
      " [0.19149974 0.80850023]]\n",
      "tf.Tensor(7.1930757, shape=(), dtype=float32)\n",
      "tf.Tensor(8.051295, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "noise = tf.random.normal([5, 100])\n",
    "fake_seqs = generator(noise, training=False)\n",
    "\n",
    "fake_prediction = discriminator.predict([fake_seqs[:,:15,:]])\n",
    "real_prediction = discriminator.predict([X1[:5]])\n",
    "print(fake_prediction)\n",
    "print(real_prediction)\n",
    "\n",
    "# disc_loss = discriminator_loss(Y[:5], real_prediction)\n",
    "gen_loss = generator_loss(fake_prediction)\n",
    "print(gen_loss)\n",
    "disc_loss = discriminator_loss(real_prediction, fake_prediction)\n",
    "print(disc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(peptides):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_bpf = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(peptides, training=True)\n",
    "      fake_output = discriminator(generated_bpf[:,:15,:], training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for seq_batch in dataset:\n",
    "      train_step(seq_batch)\n",
    "\n",
    "    # Produce images for the GIF as you go\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 1 is 3.6060619354248047 sec\n",
      "Time for epoch 2 is 0.640122652053833 sec\n",
      "Time for epoch 3 is 0.607396125793457 sec\n",
      "Time for epoch 4 is 0.3804740905761719 sec\n",
      "Time for epoch 5 is 0.3364682197570801 sec\n",
      "Time for epoch 6 is 0.46462416648864746 sec\n",
      "Time for epoch 7 is 0.552495002746582 sec\n",
      "Time for epoch 8 is 0.8124563694000244 sec\n",
      "Time for epoch 9 is 0.3494689464569092 sec\n",
      "Time for epoch 10 is 0.5501284599304199 sec\n",
      "Time for epoch 11 is 0.5289163589477539 sec\n",
      "Time for epoch 12 is 0.5576207637786865 sec\n",
      "Time for epoch 13 is 0.4158895015716553 sec\n",
      "Time for epoch 14 is 0.8341262340545654 sec\n",
      "Time for epoch 15 is 1.0258681774139404 sec\n",
      "Time for epoch 16 is 0.9606630802154541 sec\n",
      "Time for epoch 17 is 1.093747615814209 sec\n",
      "Time for epoch 18 is 1.3936424255371094 sec\n",
      "Time for epoch 19 is 1.2460801601409912 sec\n",
      "Time for epoch 20 is 0.5324950218200684 sec\n",
      "Time for epoch 21 is 0.5579359531402588 sec\n",
      "Time for epoch 22 is 0.6051826477050781 sec\n",
      "Time for epoch 23 is 0.7200794219970703 sec\n",
      "Time for epoch 24 is 0.8936207294464111 sec\n",
      "Time for epoch 25 is 0.496441125869751 sec\n",
      "Time for epoch 26 is 1.0494837760925293 sec\n",
      "Time for epoch 27 is 0.7640895843505859 sec\n",
      "Time for epoch 28 is 0.9412474632263184 sec\n",
      "Time for epoch 29 is 0.9578402042388916 sec\n",
      "Time for epoch 30 is 0.9134721755981445 sec\n",
      "Time for epoch 31 is 1.0510380268096924 sec\n",
      "Time for epoch 32 is 0.648728609085083 sec\n",
      "Time for epoch 33 is 0.9900317192077637 sec\n",
      "Time for epoch 34 is 0.925492525100708 sec\n",
      "Time for epoch 35 is 0.8774886131286621 sec\n",
      "Time for epoch 36 is 0.8142833709716797 sec\n",
      "Time for epoch 37 is 0.928253173828125 sec\n",
      "Time for epoch 38 is 1.0413851737976074 sec\n",
      "Time for epoch 39 is 0.5973474979400635 sec\n",
      "Time for epoch 40 is 0.6239783763885498 sec\n",
      "Time for epoch 41 is 0.5953481197357178 sec\n",
      "Time for epoch 42 is 0.6210775375366211 sec\n",
      "Time for epoch 43 is 0.4962773323059082 sec\n",
      "Time for epoch 44 is 0.5270473957061768 sec\n",
      "Time for epoch 45 is 0.8728890419006348 sec\n",
      "Time for epoch 46 is 0.49282097816467285 sec\n",
      "Time for epoch 47 is 0.507659912109375 sec\n",
      "Time for epoch 48 is 0.8667118549346924 sec\n",
      "Time for epoch 49 is 0.8491034507751465 sec\n",
      "Time for epoch 50 is 0.9291751384735107 sec\n"
     ]
    }
   ],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.9108116, shape=(), dtype=float32)\n",
      "[None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "noise = tf.random.normal([5, 100])\n",
    "#, tf.GradientTape() as disc_tape\n",
    "peptides = X1[:5, :15, :]\n",
    "with tf.GradientTape() as gen_tape:\n",
    "    generated_bpf = generator(noise, training=True)\n",
    "\n",
    "    # real_output = discriminator(peptides, training=True)\n",
    "    fake_output = discriminator(generated_bpf[:,:15,:], training=True)\n",
    "\n",
    "    gen_loss = generator_loss(fake_output)\n",
    "    # disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "    print(gen_loss)\n",
    "    # print(disc_loss)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    # gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    print(gradients_of_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.68451047 0.5047946  0.74413264 0.67703605 0.6957978  0.4955305\n",
      "  0.3093026  0.4925447  0.31566343 0.5124231  0.4425264  0.5000091\n",
      "  0.42750183 0.653235   0.44780466 0.63958216 0.6513634  0.32214785\n",
      "  0.591771   0.46277422]\n",
      " [0.19686647 0.5235408  0.6679461  0.37176424 0.5139134  0.38612974\n",
      "  0.5833974  0.2920839  0.36459178 0.5819297  0.4758363  0.49902785\n",
      "  0.22876447 0.47125256 0.6393183  0.21300545 0.58081186 0.28430164\n",
      "  0.62339056 0.43651858]\n",
      " [0.69651693 0.6485016  0.5314197  0.5258587  0.44587508 0.67231584\n",
      "  0.6153117  0.489702   0.64680994 0.40624878 0.3103658  0.4542466\n",
      "  0.4417386  0.36703476 0.27699602 0.66781735 0.6951212  0.44777852\n",
      "  0.8215833  0.7288543 ]\n",
      " [0.49689484 0.6939338  0.4545324  0.7042392  0.64587253 0.3566483\n",
      "  0.39660743 0.6670516  0.38430947 0.4760153  0.34967077 0.60703844\n",
      "  0.38835892 0.37492418 0.59543157 0.21528035 0.5436811  0.52745515\n",
      "  0.6851465  0.5640824 ]\n",
      " [0.58024496 0.401263   0.17889473 0.40008113 0.5205328  0.58030105\n",
      "  0.64176136 0.4308761  0.29177535 0.6665951  0.54907054 0.4858765\n",
      "  0.35934833 0.44435188 0.45134073 0.46869737 0.4950385  0.56208\n",
      "  0.59387827 0.78766996]\n",
      " [0.4285057  0.2973194  0.72629875 0.51573634 0.74897873 0.4192409\n",
      "  0.34259984 0.705163   0.38531414 0.7774483  0.55020815 0.28705022\n",
      "  0.43809894 0.55783373 0.505626   0.5887961  0.5515901  0.38775688\n",
      "  0.3839109  0.2942949 ]\n",
      " [0.5100543  0.66925573 0.2984791  0.6454851  0.52790296 0.33771843\n",
      "  0.5846831  0.6365388  0.5880733  0.48025957 0.39359117 0.6417174\n",
      "  0.6708113  0.46990895 0.5642062  0.42095277 0.4783951  0.61290777\n",
      "  0.6657029  0.6936064 ]\n",
      " [0.53283334 0.46676725 0.2190085  0.51521283 0.2841367  0.37263602\n",
      "  0.2947788  0.74565387 0.49949884 0.6374633  0.6853717  0.631237\n",
      "  0.5889597  0.63114625 0.47193322 0.46840063 0.5486598  0.6305223\n",
      "  0.5216064  0.5285954 ]\n",
      " [0.3435659  0.6535151  0.36778834 0.36877337 0.52105665 0.68947995\n",
      "  0.56194484 0.5354884  0.5163716  0.7892189  0.20589776 0.26606825\n",
      "  0.43784952 0.7557508  0.39216438 0.28849006 0.260326   0.20806958\n",
      "  0.5605268  0.35649848]\n",
      " [0.5211203  0.6883393  0.3560016  0.6023862  0.38089845 0.1829001\n",
      "  0.29551497 0.3937589  0.6403781  0.5412025  0.78021353 0.4329514\n",
      "  0.60860175 0.58170915 0.61407834 0.461247   0.3124605  0.6861555\n",
      "  0.49426314 0.628642  ]\n",
      " [0.19446073 0.65431553 0.530576   0.21298516 0.19622739 0.42641655\n",
      "  0.52719814 0.33153057 0.46782142 0.4881739  0.39199033 0.4843701\n",
      "  0.66457886 0.33632743 0.3096528  0.25720638 0.5321797  0.6471797\n",
      "  0.4342342  0.5827599 ]\n",
      " [0.33699068 0.5233932  0.62310874 0.5095102  0.34758782 0.6122633\n",
      "  0.57143    0.5406747  0.6520536  0.44049755 0.8300751  0.6561855\n",
      "  0.5219074  0.3481571  0.7437062  0.52099615 0.43017265 0.2883777\n",
      "  0.63734865 0.478635  ]\n",
      " [0.41680577 0.19620019 0.5200677  0.5409866  0.526353   0.51718867\n",
      "  0.52072936 0.75039864 0.39089933 0.5723264  0.70897293 0.37497076\n",
      "  0.7580956  0.5101488  0.31517187 0.27155772 0.43176818 0.49258116\n",
      "  0.61974823 0.3537692 ]\n",
      " [0.5533936  0.5885399  0.40768746 0.712756   0.33797187 0.48602664\n",
      "  0.49196953 0.554071   0.6946852  0.29030272 0.27578247 0.7333166\n",
      "  0.58918834 0.42892554 0.2821542  0.2902348  0.20897473 0.35483828\n",
      "  0.54450756 0.5968007 ]\n",
      " [0.7455692  0.38150442 0.3688488  0.53934926 0.32560888 0.29790407\n",
      "  0.42414612 0.31057987 0.30038637 0.5802256  0.48269004 0.4208872\n",
      "  0.76319003 0.17741902 0.5456579  0.6423853  0.4613422  0.5366806\n",
      "  0.47524023 0.3870875 ]\n",
      " [0.5251598  0.42570406 0.42632    0.5565441  0.19439507 0.5802539\n",
      "  0.39848244 0.6343885  0.63078785 0.60634965 0.33117843 0.727694\n",
      "  0.311099   0.30683413 0.70828664 0.35149083 0.6493337  0.675249\n",
      "  0.6952518  0.7670946 ]\n",
      " [0.50469595 0.32418516 0.291519   0.61513007 0.7972114  0.45794517\n",
      "  0.69261414 0.54982406 0.58243185 0.33226755 0.46292254 0.58867335\n",
      "  0.49608216 0.43796209 0.7468808  0.39818087 0.5694596  0.5290039\n",
      "  0.3712191  0.6291606 ]\n",
      " [0.45320436 0.6249234  0.21969713 0.5585152  0.16437311 0.5924223\n",
      "  0.3929568  0.66124064 0.4750814  0.40810373 0.4592778  0.39976674\n",
      "  0.49930906 0.24923907 0.24760789 0.37294254 0.47480252 0.41267076\n",
      "  0.6968072  0.4589733 ]\n",
      " [0.7014854  0.39086068 0.7232441  0.5757118  0.45045948 0.5473298\n",
      "  0.6430489  0.34047505 0.37484837 0.40505478 0.62118655 0.54403657\n",
      "  0.6379922  0.7139321  0.3242863  0.5821675  0.51562816 0.5948653\n",
      "  0.49448684 0.5839267 ]\n",
      " [0.3260497  0.6595689  0.5598187  0.189178   0.25700417 0.3244025\n",
      "  0.21647097 0.4768271  0.6833985  0.46384916 0.4412111  0.29274222\n",
      "  0.47882858 0.4542395  0.34153944 0.6134659  0.84811366 0.37805197\n",
      "  0.35324574 0.14810026]\n",
      " [0.5329024  0.37925023 0.65301335 0.4807214  0.3928811  0.73731315\n",
      "  0.34694228 0.73242074 0.5816251  0.70711535 0.5501274  0.19050398\n",
      "  0.32203558 0.59588814 0.5523452  0.56551266 0.6456236  0.3503711\n",
      "  0.6607334  0.51606166]\n",
      " [0.51246285 0.5288387  0.51136386 0.5997477  0.50556946 0.71991307\n",
      "  0.5833425  0.510507   0.5661824  0.51109123 0.6167367  0.57369536\n",
      "  0.60820496 0.5197961  0.56825745 0.44091073 0.71779203 0.4638939\n",
      "  0.6273273  0.52036816]\n",
      " [0.44201082 0.5418734  0.5173464  0.43274218 0.63378006 0.62406063\n",
      "  0.30107367 0.59800804 0.39304495 0.47362477 0.60600203 0.28229117\n",
      "  0.4578384  0.6398968  0.40293562 0.7435022  0.52176476 0.5069248\n",
      "  0.34438866 0.28752857]\n",
      " [0.66305727 0.6587924  0.4314991  0.64108145 0.57163304 0.364675\n",
      "  0.60663474 0.3910951  0.4661342  0.6227172  0.5926566  0.44960606\n",
      "  0.5263374  0.43012583 0.622393   0.5172181  0.53393215 0.36247405\n",
      "  0.3737352  0.42970467]\n",
      " [0.36518225 0.80960864 0.6210903  0.7685019  0.3658983  0.6208367\n",
      "  0.29959077 0.6648541  0.67063487 0.31239414 0.4853566  0.6320575\n",
      "  0.5918124  0.6607601  0.51052064 0.20804545 0.70803994 0.7807089\n",
      "  0.35463426 0.41535544]], shape=(25, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "noise = tf.random.normal([100, 100])\n",
    "\n",
    "generated_bpf = generator(noise, training=True)\n",
    "\n",
    "fake_output = discriminator(generated_bpf[:,:15,:], training=True)\n",
    "\n",
    "for key, output in enumerate(fake_output):\n",
    "    if output[1] > 0.8:\n",
    "        print(generated_bpf[key])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
