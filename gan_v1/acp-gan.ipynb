{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(376, 15, 20)\n",
      "(376, 2)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical as labelEncoding \n",
    "\n",
    "T = 15 # terminus_length\n",
    "\n",
    "X1 = np.load('bpf-740.npy')\n",
    "\n",
    "\n",
    "X1 = X1[:376,:T,:]\n",
    "\n",
    "\n",
    "Y  = [1 for _ in range(376)]\n",
    "\n",
    "Y = labelEncoding(Y, dtype=int)\n",
    "\n",
    "print(X1.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(376, 15, 20)\n",
      "(15, 20)\n",
      "[[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X1.shape)\n",
    "print(X1[0].shape)\n",
    "print(X1[0])\n",
    "\n",
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(X1).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're using TF-2.11.0.\n"
     ]
    }
   ],
   "source": [
    "# Deep Neural Networks:\n",
    "import tensorflow as tf; print('We\\'re using TF-{}.'.format(tf.__version__))\n",
    "# import keras; print('We\\'re using Keras-{}.'.format(keras.__version__))\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, Flatten, BatchNormalization,\n",
    "                                     Conv1D, Conv2D, MaxPooling1D, MaxPooling2D,\n",
    "                                     LSTM, GRU, Embedding, Bidirectional, Concatenate)\n",
    "from tensorflow.keras.regularizers import (l1, l2, l1_l2)\n",
    "from tensorflow.keras.optimizers import (RMSprop, Adam, SGD)\n",
    "from tensorflow.keras.models import (Sequential, Model)\n",
    "\n",
    "# Core:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Performance:\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, matthews_corrcoef, precision_score, roc_curve, auc)\n",
    "from sklearn.model_selection import (StratifiedKFold, KFold, train_test_split)\n",
    "\n",
    "#Utilities:\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical as labelEncoding   # Usages: Y = labelEncoding(Y, dtype=int)\n",
    "from tensorflow.keras.utils import plot_model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    ### Head-1:\n",
    "    input1 = Input(shape=X1[0].shape)\n",
    "\n",
    "    x = Conv1D(filters=10, kernel_size=4, padding='same', activation='relu', kernel_regularizer=l2(l=0.01))(input1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.80)(x)\n",
    "\n",
    "    x = Conv1D(filters=8, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(l=0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.70)(x)\n",
    "\n",
    "    head1 = Flatten()(x)\n",
    "\n",
    "    # merge\n",
    "    merge = Concatenate()([head1])\n",
    "\n",
    "    output = Dense(units=8, activation='relu', kernel_regularizer=l2(l=0.01))(merge)\n",
    "    output = BatchNormalization()(output)\n",
    "    output = Dropout(rate=0.70)(output)\n",
    "\n",
    "    output = Dense(units=2, activation='softmax')(output)\n",
    "\n",
    "    return Model(inputs=[input1], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "from Bio.SeqUtils import ProtParam\n",
    "from Bio.Align import substitution_matrices\n",
    "from Bio.SubsMat import MatrixInfo\n",
    "\n",
    "\n",
    "# Function to convert binary profile feature to amino acid sequence\n",
    "def bpf_to_sequence(binary_profile_feature):\n",
    "    amino_acids = \"ARNDCQEGHILKMFPSTWYVBZX\"\n",
    "    sequence = \"\"\n",
    "    for row in binary_profile_feature:\n",
    "        index = row.argmax()\n",
    "        sequence += amino_acids[index]\n",
    "    return sequence\n",
    "\n",
    "def seq_to_blosum(sequence):\n",
    "    blosum62 = MatrixInfo.blosum62\n",
    "    sequence = sequence.upper()\n",
    "    length = len(sequence)\n",
    "    blosum_matrix = []\n",
    "    for aa1 in sequence:\n",
    "        row = []\n",
    "        for aa2 in sequence:\n",
    "            if (aa1, aa2) in blosum62:\n",
    "                row.append(blosum62[(aa1, aa2)])\n",
    "            else:\n",
    "                row.append(blosum62[(aa2, aa1)])\n",
    "        blosum_matrix.append(row)\n",
    "    return blosum_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpf_to_sequence(binary_profile_feature):\n",
    "    amino_acids = \"ARNDCQEGHILKMFPSTWYVBZX\"\n",
    "    sequence = \"\"\n",
    "    for row in binary_profile_feature:\n",
    "        i = 0\n",
    "        for item in row:\n",
    "            i += 1\n",
    "            if item.numpy() == 1:\n",
    "                sequence += amino_acids[i]\n",
    "    return sequence\n",
    "\n",
    "def seq_to_blosum(sequence):\n",
    "    blosum62 = MatrixInfo.blosum62\n",
    "    sequence = sequence.upper()\n",
    "    length = len(sequence)\n",
    "    blosum_matrix = []\n",
    "    for aa1 in sequence:\n",
    "        row = []\n",
    "        for aa2 in sequence:\n",
    "            if (aa1, aa2) in blosum62:\n",
    "                row.append(blosum62[(aa1, aa2)])\n",
    "            else:\n",
    "                row.append(blosum62[(aa2, aa1)])\n",
    "        blosum_matrix.append(row[:20])\n",
    "    return blosum_matrix\n",
    "\n",
    "def bpf_to_blosum_layer(seqs_bpf):\n",
    "    seqs_blosum = []\n",
    "    for bpf in seqs_bpf:\n",
    "        str_seq = bpf_to_sequence(bpf)\n",
    "        seqs_blosum.append(seq_to_blosum(str_seq))\n",
    "\n",
    "    print(seqs_blosum)\n",
    "    return seqs_blosum\n",
    "\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Dense(512))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Dense(25 * 20, activation='sigmoid'))\n",
    "    model.add(layers.Reshape((25, 20)))\n",
    "    return model\n",
    "    \n",
    "\n",
    "    # model = Sequential([\n",
    "    #     layers.Input(shape=(latent_dim,)),\n",
    "    #     layers.Dense(128, activation='relu'),\n",
    "    #     layers.Dense(256, activation='relu'),\n",
    "    #     layers.Dense(output_shape[0] * output_shape[1], activation='sigmoid'),\n",
    "    #     layers.Reshape(output_shape),\n",
    "    #     layers.Lambda(lambda x: tf.one_hot(tf.argmax(x, axis=-1), depth=output_shape[1])),\n",
    "    #     layers.Lambda(lambda x: tf.cast(x, dtype=tf.int32)),\n",
    "    # ])\n",
    "\n",
    "    # return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 25, 20), dtype=float32, numpy=\n",
       "array([[[0.6183278 , 0.5159866 , 0.56070536, 0.48719716, 0.36398685,\n",
       "         0.46603346, 0.462894  , 0.63001984, 0.3109427 , 0.47952646,\n",
       "         0.4812577 , 0.68320346, 0.494835  , 0.4279682 , 0.5214799 ,\n",
       "         0.48720098, 0.61891735, 0.6444756 , 0.4186665 , 0.56531405],\n",
       "        [0.63043535, 0.48973018, 0.46949843, 0.61755186, 0.34962735,\n",
       "         0.5789172 , 0.38743177, 0.52229524, 0.6086862 , 0.552657  ,\n",
       "         0.6451335 , 0.44995406, 0.37592936, 0.4265271 , 0.47425592,\n",
       "         0.38870603, 0.4174372 , 0.45291737, 0.4855664 , 0.5406489 ],\n",
       "        [0.41758332, 0.4631627 , 0.7293058 , 0.52312624, 0.48558998,\n",
       "         0.609188  , 0.5070556 , 0.44890413, 0.3314407 , 0.5724512 ,\n",
       "         0.3756082 , 0.32442918, 0.62455094, 0.57630205, 0.6044108 ,\n",
       "         0.42950892, 0.6108189 , 0.40798673, 0.5896743 , 0.4006242 ],\n",
       "        [0.4874911 , 0.43888655, 0.49013484, 0.5223715 , 0.5621511 ,\n",
       "         0.4906344 , 0.42476758, 0.5465541 , 0.3895912 , 0.645054  ,\n",
       "         0.46741548, 0.528149  , 0.37419006, 0.4228256 , 0.55013156,\n",
       "         0.33532086, 0.3274612 , 0.6937739 , 0.37426424, 0.510843  ],\n",
       "        [0.4197969 , 0.42455062, 0.4994564 , 0.52457917, 0.35957927,\n",
       "         0.41834104, 0.55286014, 0.41520327, 0.6559038 , 0.6322585 ,\n",
       "         0.5875002 , 0.5756166 , 0.40488923, 0.45410624, 0.42457303,\n",
       "         0.36660567, 0.4460809 , 0.63894355, 0.61849487, 0.43389407],\n",
       "        [0.37605056, 0.55888206, 0.46402317, 0.3513671 , 0.3895923 ,\n",
       "         0.49367654, 0.5080488 , 0.4415667 , 0.590747  , 0.6561415 ,\n",
       "         0.31288815, 0.47387832, 0.3319263 , 0.5998207 , 0.63095266,\n",
       "         0.6261965 , 0.5741412 , 0.4110794 , 0.409792  , 0.66452104],\n",
       "        [0.5342937 , 0.4528055 , 0.5455803 , 0.48643637, 0.44005704,\n",
       "         0.50695354, 0.3631586 , 0.59779626, 0.45746264, 0.43145716,\n",
       "         0.45437202, 0.5111597 , 0.576528  , 0.54459697, 0.43741125,\n",
       "         0.5876218 , 0.45136142, 0.47661117, 0.6900161 , 0.48277792],\n",
       "        [0.44857314, 0.33081743, 0.35080627, 0.5890186 , 0.3954053 ,\n",
       "         0.52925014, 0.63025904, 0.4380009 , 0.65183926, 0.37978062,\n",
       "         0.31329453, 0.6012202 , 0.407655  , 0.42482042, 0.5716982 ,\n",
       "         0.6817692 , 0.40998185, 0.5627552 , 0.38967615, 0.6122816 ],\n",
       "        [0.49821213, 0.47521305, 0.5509508 , 0.5513637 , 0.44597635,\n",
       "         0.48002258, 0.42802995, 0.51876265, 0.5803962 , 0.5293224 ,\n",
       "         0.5443636 , 0.6354169 , 0.44390652, 0.51139784, 0.67127174,\n",
       "         0.6079484 , 0.5976618 , 0.5032039 , 0.51457214, 0.5237231 ],\n",
       "        [0.6124052 , 0.46993113, 0.5088358 , 0.5971774 , 0.47779647,\n",
       "         0.46556866, 0.5491761 , 0.51708776, 0.31870916, 0.45241746,\n",
       "         0.612855  , 0.5262219 , 0.51215327, 0.3600438 , 0.3756031 ,\n",
       "         0.32980132, 0.45788828, 0.4780349 , 0.50822985, 0.32180065],\n",
       "        [0.52797383, 0.5653216 , 0.47476128, 0.4953815 , 0.5095855 ,\n",
       "         0.45735455, 0.45744783, 0.3798254 , 0.47113943, 0.45416224,\n",
       "         0.44374865, 0.49983844, 0.42900497, 0.3966181 , 0.42937058,\n",
       "         0.59026456, 0.4950969 , 0.47779697, 0.5977021 , 0.61231804],\n",
       "        [0.47033215, 0.5386007 , 0.35670888, 0.5384752 , 0.4901281 ,\n",
       "         0.51903534, 0.4619246 , 0.4442951 , 0.6429309 , 0.670673  ,\n",
       "         0.54746366, 0.4884316 , 0.45595026, 0.69807124, 0.50599295,\n",
       "         0.3890419 , 0.552852  , 0.42902973, 0.65565217, 0.5928998 ],\n",
       "        [0.39521772, 0.29542762, 0.4914117 , 0.39192632, 0.40265667,\n",
       "         0.590335  , 0.5340838 , 0.45899197, 0.39666307, 0.4517088 ,\n",
       "         0.56319237, 0.5975642 , 0.51165694, 0.6253066 , 0.4204776 ,\n",
       "         0.5325487 , 0.38796735, 0.563749  , 0.4861598 , 0.47205287],\n",
       "        [0.48704663, 0.49105024, 0.49631774, 0.3193753 , 0.49491554,\n",
       "         0.6272455 , 0.5334443 , 0.47720507, 0.5575195 , 0.56100917,\n",
       "         0.3460332 , 0.40569225, 0.6064217 , 0.54652   , 0.43376473,\n",
       "         0.44624603, 0.5708086 , 0.509699  , 0.42440802, 0.5310123 ],\n",
       "        [0.51465344, 0.6123819 , 0.5879564 , 0.5870901 , 0.55458504,\n",
       "         0.36046052, 0.5441523 , 0.48227566, 0.62326854, 0.62901   ,\n",
       "         0.47804433, 0.56637144, 0.5963341 , 0.49741143, 0.63922113,\n",
       "         0.5201212 , 0.37332946, 0.517757  , 0.34779325, 0.50580835],\n",
       "        [0.52750653, 0.48523027, 0.4079947 , 0.45025888, 0.36710578,\n",
       "         0.5756248 , 0.38696808, 0.4201124 , 0.36502233, 0.4008949 ,\n",
       "         0.5131808 , 0.6531087 , 0.5817742 , 0.54654974, 0.49687546,\n",
       "         0.6282528 , 0.56535923, 0.5418729 , 0.55130744, 0.55399215],\n",
       "        [0.43939492, 0.6075845 , 0.5256718 , 0.48851714, 0.5640021 ,\n",
       "         0.566794  , 0.55471575, 0.40502685, 0.38352185, 0.54947793,\n",
       "         0.61302227, 0.5578937 , 0.46592268, 0.39530814, 0.4941496 ,\n",
       "         0.5082096 , 0.43594417, 0.4755067 , 0.4934987 , 0.65338975],\n",
       "        [0.35499984, 0.50051296, 0.42295918, 0.363061  , 0.5791598 ,\n",
       "         0.72215176, 0.58940196, 0.49625877, 0.40238348, 0.4575486 ,\n",
       "         0.4661245 , 0.5404588 , 0.47384366, 0.44793713, 0.45233443,\n",
       "         0.30836833, 0.61757386, 0.39869708, 0.63762873, 0.3831478 ],\n",
       "        [0.4467573 , 0.43446302, 0.45848873, 0.3910804 , 0.47344595,\n",
       "         0.41807774, 0.69503057, 0.52409905, 0.58809227, 0.5106045 ,\n",
       "         0.36464277, 0.43740463, 0.44763693, 0.43664467, 0.34987175,\n",
       "         0.37588045, 0.49638024, 0.41825268, 0.5740756 , 0.4909181 ],\n",
       "        [0.4729482 , 0.3735561 , 0.6426852 , 0.2888829 , 0.49827337,\n",
       "         0.54745436, 0.5228523 , 0.34137726, 0.40602   , 0.4680859 ,\n",
       "         0.49238712, 0.40268782, 0.5057118 , 0.54818434, 0.41137075,\n",
       "         0.5260726 , 0.5483949 , 0.4658262 , 0.59689105, 0.38336954],\n",
       "        [0.55867773, 0.4965617 , 0.50030833, 0.4081624 , 0.35602117,\n",
       "         0.44042984, 0.46172956, 0.42863536, 0.35191143, 0.59187   ,\n",
       "         0.4850475 , 0.45443103, 0.61636174, 0.5299254 , 0.5978039 ,\n",
       "         0.3714675 , 0.44548818, 0.5975425 , 0.5311881 , 0.5056169 ],\n",
       "        [0.48877102, 0.33142275, 0.4698892 , 0.36107397, 0.3909485 ,\n",
       "         0.4178602 , 0.50529075, 0.552867  , 0.6697778 , 0.47509664,\n",
       "         0.5971599 , 0.47127244, 0.38735786, 0.54818726, 0.57396346,\n",
       "         0.55610013, 0.45066276, 0.40975577, 0.31466186, 0.5595068 ],\n",
       "        [0.50505525, 0.4893766 , 0.5750941 , 0.59631276, 0.57044774,\n",
       "         0.60264444, 0.52440166, 0.6026518 , 0.6031306 , 0.606151  ,\n",
       "         0.5581718 , 0.5432694 , 0.42506608, 0.54184324, 0.5797768 ,\n",
       "         0.46739075, 0.5002311 , 0.7298846 , 0.42787606, 0.5408097 ],\n",
       "        [0.45132107, 0.60165876, 0.6048268 , 0.45990655, 0.50579643,\n",
       "         0.4645385 , 0.61351514, 0.5809858 , 0.56867194, 0.44202277,\n",
       "         0.5422664 , 0.49122268, 0.40485954, 0.5297827 , 0.50480276,\n",
       "         0.36795756, 0.66760206, 0.510927  , 0.5064549 , 0.5715024 ],\n",
       "        [0.6873642 , 0.5745372 , 0.57556266, 0.34587246, 0.4930994 ,\n",
       "         0.36056763, 0.46571532, 0.65798163, 0.38437924, 0.26848918,\n",
       "         0.5475473 , 0.43224046, 0.41399935, 0.6077289 , 0.6780428 ,\n",
       "         0.48825493, 0.29299587, 0.5322927 , 0.41428748, 0.44054753]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the dimensions\n",
    "latent_dim = 100  # Dimensionality of the latent space\n",
    "batch_size = 1\n",
    "output_shape = (25, 20)  # Desired output shape of the matrix\n",
    "\n",
    "generator = build_generator()\n",
    "\n",
    "noise = tf.random.normal([batch_size, latent_dim])\n",
    "\n",
    "generated_matrix = generator(noise, training=False)\n",
    "\n",
    "generated_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_discriminator()\n",
    "discriminator.load_weights('./acp_mhcnn_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 112ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.30780783, 0.6921922 ],\n",
       "       [0.17504199, 0.824958  ],\n",
       "       [0.31832418, 0.68167585],\n",
       "       [0.18360284, 0.81639713],\n",
       "       [0.19149974, 0.80850023]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = discriminator.predict([X1[:5,:,:]])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 4ms/step\n",
      "[[9.9999881e-01 1.2514822e-06]\n",
      " [9.9999964e-01 3.7884419e-07]\n",
      " [9.9999928e-01 7.3201142e-07]\n",
      " ...\n",
      " [9.9999940e-01 6.5050284e-07]\n",
      " [9.9999964e-01 3.9253493e-07]\n",
      " [9.9999952e-01 4.6214348e-07]]\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "noise = tf.random.normal([1000, 100])\n",
    "fake_seqs = generator(noise, training=False)\n",
    "\n",
    "prediction = discriminator.predict([fake_seqs[:,:15,:]])\n",
    "print(prediction)\n",
    "\n",
    "over50 = 0\n",
    "over25 = 0\n",
    "over40 = 0\n",
    "total = 0\n",
    "for key, output in enumerate(prediction):\n",
    "    if output[1] > 0.25:\n",
    "        over25 += 1\n",
    "    if output[1] > 0.4:\n",
    "        over40 += 1\n",
    "    if output[1] > 0.5:\n",
    "        over50 += 1\n",
    "    total += 1\n",
    "\n",
    "print(over25 / total)\n",
    "print(over40 / total)\n",
    "print(over50 / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.2717897>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    ones_second_column = tf.ones_like(real_output[:, 1:2])\n",
    "    zeros_first_column = tf.zeros_like(real_output[:, :1])\n",
    "    custom_matrix_real = tf.concat([zeros_first_column, ones_second_column], axis=1)\n",
    "    ones_second_column = tf.ones_like(fake_output[:, 1:2])\n",
    "    zeros_first_column = tf.zeros_like(fake_output[:, :1])\n",
    "    custom_matrix_fake = tf.concat([ones_second_column, zeros_first_column], axis=1)\n",
    "\n",
    "    real_loss = tf.keras.losses.BinaryCrossentropy()(custom_matrix_real, real_output)\n",
    "    fake_loss = tf.keras.losses.BinaryCrossentropy()(custom_matrix_fake, fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "discriminator_loss(discriminator.predict([X1[:5,:,:]]), prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=13.90133>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generator_loss(fake_output):\n",
    "    ones_second_column = tf.ones_like(fake_output[:, 1:2])\n",
    "    zeros_first_column = tf.zeros_like(fake_output[:, :1])\n",
    "    custom_matrix = tf.concat([zeros_first_column, ones_second_column], axis=1)\n",
    "    return tf.keras.losses.BinaryCrossentropy()(custom_matrix, fake_output)\n",
    "\n",
    "generator_loss(prediction)\n",
    "# def generator_loss(disc_output):\n",
    "#     batch_size = tf.shape(disc_output)[0]\n",
    "#     num_classes = tf.shape(disc_output)[1]\n",
    "#     desired_output = tf.concat([tf.zeros((batch_size, 1)), tf.ones((batch_size, num_classes - 1))], axis=1)\n",
    "    \n",
    "#     # Define binary cross entropy loss\n",
    "#     bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    \n",
    "#     # Calculate loss\n",
    "#     loss = bce(desired_output, disc_output)\n",
    "    \n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "[[9.9999726e-01 2.7330827e-06]\n",
      " [9.9999952e-01 4.3494552e-07]\n",
      " [9.9999928e-01 7.1344823e-07]\n",
      " [9.9999952e-01 5.2333223e-07]\n",
      " [9.9999952e-01 4.3110580e-07]]\n",
      "[[0.30780783 0.6921922 ]\n",
      " [0.17504199 0.824958  ]\n",
      " [0.31832418 0.68167585]\n",
      " [0.18360284 0.81639713]\n",
      " [0.19149974 0.80850023]]\n",
      "tf.Tensor(13.986166, shape=(), dtype=float32)\n",
      "tf.Tensor(0.27178952, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "noise = tf.random.normal([5, 100])\n",
    "fake_seqs = generator(noise, training=False)\n",
    "\n",
    "fake_prediction = discriminator.predict([fake_seqs[:,:15,:]])\n",
    "real_prediction = discriminator.predict([X1[:5]])\n",
    "print(fake_prediction)\n",
    "print(real_prediction)\n",
    "\n",
    "# disc_loss = discriminator_loss(Y[:5], real_prediction)\n",
    "gen_loss = generator_loss(fake_prediction)\n",
    "print(gen_loss)\n",
    "disc_loss = discriminator_loss(real_prediction, fake_prediction)\n",
    "print(disc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "EPOCHS = 500\n",
    "noise_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(peptides):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_bpf = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(peptides, training=True)\n",
    "      fake_output = discriminator(generated_bpf[:,:15,:], training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    print(gen_loss)\n",
    "    print(disc_loss)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "def train(epochs):\n",
    "  for epoch in range(epochs):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X1).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for seq_batch in dataset:\n",
    "      train_step(seq_batch)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"binary_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
      "Tensor(\"add:0\", shape=(), dtype=float32)\n",
      "Tensor(\"binary_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
      "Tensor(\"add:0\", shape=(), dtype=float32)\n",
      "Tensor(\"binary_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
      "Tensor(\"add:0\", shape=(), dtype=float32)\n",
      "Time for epoch 1 is 4.874107837677002 sec\n",
      "Time for epoch 2 is 0.5278592109680176 sec\n",
      "Time for epoch 3 is 0.46424317359924316 sec\n",
      "Time for epoch 4 is 0.34880709648132324 sec\n",
      "Time for epoch 5 is 0.4910550117492676 sec\n",
      "Time for epoch 6 is 0.3827323913574219 sec\n",
      "Time for epoch 7 is 0.48107385635375977 sec\n",
      "Time for epoch 8 is 0.39923524856567383 sec\n",
      "Time for epoch 9 is 0.631192684173584 sec\n",
      "Time for epoch 10 is 0.3398001194000244 sec\n",
      "Time for epoch 11 is 0.4551985263824463 sec\n",
      "Time for epoch 12 is 0.366527795791626 sec\n",
      "Time for epoch 13 is 0.5780384540557861 sec\n",
      "Time for epoch 14 is 0.7999856472015381 sec\n",
      "Time for epoch 15 is 0.9407942295074463 sec\n",
      "Time for epoch 16 is 0.7216577529907227 sec\n",
      "Time for epoch 17 is 0.7006669044494629 sec\n",
      "Time for epoch 18 is 0.5734603404998779 sec\n",
      "Time for epoch 19 is 0.2732505798339844 sec\n",
      "Time for epoch 20 is 0.3706798553466797 sec\n",
      "Time for epoch 21 is 0.47121262550354004 sec\n",
      "Time for epoch 22 is 0.33153748512268066 sec\n",
      "Time for epoch 23 is 0.5094592571258545 sec\n",
      "Time for epoch 24 is 0.31505918502807617 sec\n",
      "Time for epoch 25 is 0.6739203929901123 sec\n",
      "Time for epoch 26 is 0.42635488510131836 sec\n",
      "Time for epoch 27 is 0.3254828453063965 sec\n",
      "Time for epoch 28 is 0.1672360897064209 sec\n",
      "Time for epoch 29 is 0.679978609085083 sec\n",
      "Time for epoch 30 is 0.8152291774749756 sec\n",
      "Time for epoch 31 is 0.5929980278015137 sec\n",
      "Time for epoch 32 is 0.8156135082244873 sec\n",
      "Time for epoch 33 is 0.4584999084472656 sec\n",
      "Time for epoch 34 is 0.5745797157287598 sec\n",
      "Time for epoch 35 is 0.8607687950134277 sec\n",
      "Time for epoch 36 is 0.35958051681518555 sec\n",
      "Time for epoch 37 is 0.30832695960998535 sec\n",
      "Time for epoch 38 is 0.43363237380981445 sec\n",
      "Time for epoch 39 is 0.3906900882720947 sec\n",
      "Time for epoch 40 is 0.3050832748413086 sec\n",
      "Time for epoch 41 is 0.23977255821228027 sec\n",
      "Time for epoch 42 is 0.30180883407592773 sec\n",
      "Time for epoch 43 is 0.642019510269165 sec\n",
      "Time for epoch 44 is 0.27515697479248047 sec\n",
      "Time for epoch 45 is 0.41617321968078613 sec\n",
      "Time for epoch 46 is 0.7456955909729004 sec\n",
      "Time for epoch 47 is 0.4527914524078369 sec\n",
      "Time for epoch 48 is 0.6606283187866211 sec\n",
      "Time for epoch 49 is 0.482025146484375 sec\n",
      "Time for epoch 50 is 0.2636830806732178 sec\n",
      "Time for epoch 51 is 0.358227014541626 sec\n",
      "Time for epoch 52 is 0.6060965061187744 sec\n",
      "Time for epoch 53 is 0.7842071056365967 sec\n",
      "Time for epoch 54 is 0.6992909908294678 sec\n",
      "Time for epoch 55 is 0.6474776268005371 sec\n",
      "Time for epoch 56 is 0.5888895988464355 sec\n",
      "Time for epoch 57 is 0.8433346748352051 sec\n",
      "Time for epoch 58 is 0.6440398693084717 sec\n",
      "Time for epoch 59 is 0.8088552951812744 sec\n",
      "Time for epoch 60 is 1.1978414058685303 sec\n",
      "Time for epoch 61 is 0.8931288719177246 sec\n",
      "Time for epoch 62 is 0.5760691165924072 sec\n",
      "Time for epoch 63 is 0.8114643096923828 sec\n",
      "Time for epoch 64 is 0.9467098712921143 sec\n",
      "Time for epoch 65 is 0.6733975410461426 sec\n",
      "Time for epoch 66 is 0.6123900413513184 sec\n",
      "Time for epoch 67 is 0.6243572235107422 sec\n",
      "Time for epoch 68 is 0.8858523368835449 sec\n",
      "Time for epoch 69 is 0.9234871864318848 sec\n",
      "Time for epoch 70 is 0.6998391151428223 sec\n",
      "Time for epoch 71 is 0.8771219253540039 sec\n",
      "Time for epoch 72 is 1.010958194732666 sec\n",
      "Time for epoch 73 is 0.5866477489471436 sec\n",
      "Time for epoch 74 is 0.5019168853759766 sec\n",
      "Time for epoch 75 is 0.6895971298217773 sec\n",
      "Time for epoch 76 is 0.65140700340271 sec\n",
      "Time for epoch 77 is 0.5287044048309326 sec\n",
      "Time for epoch 78 is 0.4999871253967285 sec\n",
      "Time for epoch 79 is 0.7455043792724609 sec\n",
      "Time for epoch 80 is 0.7835381031036377 sec\n",
      "Time for epoch 81 is 0.6346333026885986 sec\n",
      "Time for epoch 82 is 0.5887866020202637 sec\n",
      "Time for epoch 83 is 0.6038656234741211 sec\n",
      "Time for epoch 84 is 0.3857078552246094 sec\n",
      "Time for epoch 85 is 0.4989347457885742 sec\n",
      "Time for epoch 86 is 0.5299773216247559 sec\n",
      "Time for epoch 87 is 0.4827108383178711 sec\n",
      "Time for epoch 88 is 1.0137975215911865 sec\n",
      "Time for epoch 89 is 1.0624301433563232 sec\n",
      "Time for epoch 90 is 1.0050287246704102 sec\n",
      "Time for epoch 91 is 1.037374496459961 sec\n",
      "Time for epoch 92 is 0.8856370449066162 sec\n",
      "Time for epoch 93 is 0.8262500762939453 sec\n",
      "Time for epoch 94 is 0.6461992263793945 sec\n",
      "Time for epoch 95 is 0.7492990493774414 sec\n",
      "Time for epoch 96 is 0.6834282875061035 sec\n",
      "Time for epoch 97 is 0.777578592300415 sec\n",
      "Time for epoch 98 is 0.8872852325439453 sec\n",
      "Time for epoch 99 is 0.5422244071960449 sec\n",
      "Time for epoch 100 is 0.6589620113372803 sec\n",
      "Time for epoch 101 is 0.7562892436981201 sec\n",
      "Time for epoch 102 is 0.7135357856750488 sec\n",
      "Time for epoch 103 is 0.404818058013916 sec\n",
      "Time for epoch 104 is 0.5786142349243164 sec\n",
      "Time for epoch 105 is 0.5191080570220947 sec\n",
      "Time for epoch 106 is 0.6008110046386719 sec\n",
      "Time for epoch 107 is 0.9955987930297852 sec\n",
      "Time for epoch 108 is 0.5026693344116211 sec\n",
      "Time for epoch 109 is 0.6634354591369629 sec\n",
      "Time for epoch 110 is 0.4792332649230957 sec\n",
      "Time for epoch 111 is 0.8397884368896484 sec\n",
      "Time for epoch 112 is 0.6728055477142334 sec\n",
      "Time for epoch 113 is 0.4262654781341553 sec\n",
      "Time for epoch 114 is 0.3188049793243408 sec\n",
      "Time for epoch 115 is 0.3124394416809082 sec\n",
      "Time for epoch 116 is 0.6835958957672119 sec\n",
      "Time for epoch 117 is 0.41533780097961426 sec\n",
      "Time for epoch 118 is 0.7240073680877686 sec\n",
      "Time for epoch 119 is 0.6085593700408936 sec\n",
      "Time for epoch 120 is 0.8487226963043213 sec\n",
      "Time for epoch 121 is 0.7710978984832764 sec\n",
      "Time for epoch 122 is 0.8999621868133545 sec\n",
      "Time for epoch 123 is 0.646284818649292 sec\n",
      "Time for epoch 124 is 0.6549332141876221 sec\n",
      "Time for epoch 125 is 0.495682954788208 sec\n",
      "Time for epoch 126 is 0.45641160011291504 sec\n",
      "Time for epoch 127 is 0.26751112937927246 sec\n",
      "Time for epoch 128 is 0.6229817867279053 sec\n",
      "Time for epoch 129 is 0.3875386714935303 sec\n",
      "Time for epoch 130 is 0.44553208351135254 sec\n",
      "Time for epoch 131 is 0.4579331874847412 sec\n",
      "Time for epoch 132 is 0.47759461402893066 sec\n",
      "Time for epoch 133 is 0.48267626762390137 sec\n",
      "Time for epoch 134 is 0.6729881763458252 sec\n",
      "Time for epoch 135 is 0.7996277809143066 sec\n",
      "Time for epoch 136 is 0.5807452201843262 sec\n",
      "Time for epoch 137 is 0.43944311141967773 sec\n",
      "Time for epoch 138 is 0.6979067325592041 sec\n",
      "Time for epoch 139 is 0.6874947547912598 sec\n",
      "Time for epoch 140 is 0.4876885414123535 sec\n",
      "Time for epoch 141 is 0.7437596321105957 sec\n",
      "Time for epoch 142 is 0.6199426651000977 sec\n",
      "Time for epoch 143 is 0.7219488620758057 sec\n",
      "Time for epoch 144 is 0.63173508644104 sec\n",
      "Time for epoch 145 is 0.805962085723877 sec\n",
      "Time for epoch 146 is 0.6831209659576416 sec\n",
      "Time for epoch 147 is 0.7058305740356445 sec\n",
      "Time for epoch 148 is 0.6061809062957764 sec\n",
      "Time for epoch 149 is 1.0023541450500488 sec\n",
      "Time for epoch 150 is 1.1048662662506104 sec\n",
      "Time for epoch 151 is 0.7398228645324707 sec\n",
      "Time for epoch 152 is 0.47401976585388184 sec\n",
      "Time for epoch 153 is 0.5173187255859375 sec\n",
      "Time for epoch 154 is 0.488722562789917 sec\n",
      "Time for epoch 155 is 0.3613131046295166 sec\n",
      "Time for epoch 156 is 0.8287670612335205 sec\n",
      "Time for epoch 157 is 0.6742498874664307 sec\n",
      "Time for epoch 158 is 0.5166923999786377 sec\n",
      "Time for epoch 159 is 0.5209105014801025 sec\n",
      "Time for epoch 160 is 0.46396803855895996 sec\n",
      "Time for epoch 161 is 0.3682124614715576 sec\n",
      "Time for epoch 162 is 0.37233614921569824 sec\n",
      "Time for epoch 163 is 0.33515310287475586 sec\n",
      "Time for epoch 164 is 0.8332769870758057 sec\n",
      "Time for epoch 165 is 0.855412483215332 sec\n",
      "Time for epoch 166 is 0.47646594047546387 sec\n",
      "Time for epoch 167 is 0.5792348384857178 sec\n",
      "Time for epoch 168 is 0.747340202331543 sec\n",
      "Time for epoch 169 is 0.8230462074279785 sec\n",
      "Time for epoch 170 is 0.6906337738037109 sec\n",
      "Time for epoch 171 is 0.6320469379425049 sec\n",
      "Time for epoch 172 is 0.7702887058258057 sec\n",
      "Time for epoch 173 is 0.7605607509613037 sec\n",
      "Time for epoch 174 is 0.5973401069641113 sec\n",
      "Time for epoch 175 is 0.8059875965118408 sec\n",
      "Time for epoch 176 is 0.4496035575866699 sec\n",
      "Time for epoch 177 is 0.4207899570465088 sec\n",
      "Time for epoch 178 is 0.6799771785736084 sec\n",
      "Time for epoch 179 is 0.35016751289367676 sec\n",
      "Time for epoch 180 is 0.7434070110321045 sec\n",
      "Time for epoch 181 is 0.48833465576171875 sec\n",
      "Time for epoch 182 is 0.6009621620178223 sec\n",
      "Time for epoch 183 is 0.768585205078125 sec\n",
      "Time for epoch 184 is 0.9565138816833496 sec\n",
      "Time for epoch 185 is 0.9620769023895264 sec\n",
      "Time for epoch 186 is 0.895632266998291 sec\n",
      "Time for epoch 187 is 0.8321564197540283 sec\n",
      "Time for epoch 188 is 0.9529914855957031 sec\n",
      "Time for epoch 189 is 0.9932386875152588 sec\n",
      "Time for epoch 190 is 0.5996453762054443 sec\n",
      "Time for epoch 191 is 0.5416572093963623 sec\n",
      "Time for epoch 192 is 0.4624800682067871 sec\n",
      "Time for epoch 193 is 0.6696012020111084 sec\n",
      "Time for epoch 194 is 0.8863790035247803 sec\n",
      "Time for epoch 195 is 0.7327675819396973 sec\n",
      "Time for epoch 196 is 0.3814117908477783 sec\n",
      "Time for epoch 197 is 0.41545796394348145 sec\n",
      "Time for epoch 198 is 0.5934779644012451 sec\n",
      "Time for epoch 199 is 0.7414140701293945 sec\n",
      "Time for epoch 200 is 0.7520351409912109 sec\n",
      "Time for epoch 201 is 0.7800619602203369 sec\n",
      "Time for epoch 202 is 0.6517207622528076 sec\n",
      "Time for epoch 203 is 0.6075119972229004 sec\n",
      "Time for epoch 204 is 0.7282509803771973 sec\n",
      "Time for epoch 205 is 0.9316999912261963 sec\n",
      "Time for epoch 206 is 0.6595659255981445 sec\n",
      "Time for epoch 207 is 0.7139153480529785 sec\n",
      "Time for epoch 208 is 0.6954696178436279 sec\n",
      "Time for epoch 209 is 0.3987720012664795 sec\n",
      "Time for epoch 210 is 0.9898297786712646 sec\n",
      "Time for epoch 211 is 0.9622981548309326 sec\n",
      "Time for epoch 212 is 0.8734591007232666 sec\n",
      "Time for epoch 213 is 0.922004222869873 sec\n",
      "Time for epoch 214 is 0.8920047283172607 sec\n",
      "Time for epoch 215 is 0.9627461433410645 sec\n",
      "Time for epoch 216 is 0.6935248374938965 sec\n",
      "Time for epoch 217 is 0.8071565628051758 sec\n",
      "Time for epoch 218 is 0.8589777946472168 sec\n",
      "Time for epoch 219 is 0.6698174476623535 sec\n",
      "Time for epoch 220 is 0.5303823947906494 sec\n",
      "Time for epoch 221 is 0.7623133659362793 sec\n",
      "Time for epoch 222 is 0.9563071727752686 sec\n",
      "Time for epoch 223 is 0.8265478610992432 sec\n",
      "Time for epoch 224 is 0.5485968589782715 sec\n",
      "Time for epoch 225 is 0.6607794761657715 sec\n",
      "Time for epoch 226 is 0.7427198886871338 sec\n",
      "Time for epoch 227 is 0.6586921215057373 sec\n",
      "Time for epoch 228 is 0.9093215465545654 sec\n",
      "Time for epoch 229 is 0.705312967300415 sec\n",
      "Time for epoch 230 is 1.0189428329467773 sec\n",
      "Time for epoch 231 is 0.9170691967010498 sec\n",
      "Time for epoch 232 is 0.7529449462890625 sec\n",
      "Time for epoch 233 is 0.4919266700744629 sec\n",
      "Time for epoch 234 is 0.5481133460998535 sec\n",
      "Time for epoch 235 is 0.26610827445983887 sec\n",
      "Time for epoch 236 is 0.533958911895752 sec\n",
      "Time for epoch 237 is 0.7222433090209961 sec\n",
      "Time for epoch 238 is 0.7486839294433594 sec\n",
      "Time for epoch 239 is 0.774266242980957 sec\n",
      "Time for epoch 240 is 0.9650111198425293 sec\n",
      "Time for epoch 241 is 0.7475807666778564 sec\n",
      "Time for epoch 242 is 0.5244128704071045 sec\n",
      "Time for epoch 243 is 0.7166943550109863 sec\n",
      "Time for epoch 244 is 0.7699153423309326 sec\n",
      "Time for epoch 245 is 0.4215846061706543 sec\n",
      "Time for epoch 246 is 0.49547529220581055 sec\n",
      "Time for epoch 247 is 0.5603392124176025 sec\n",
      "Time for epoch 248 is 0.7373080253601074 sec\n",
      "Time for epoch 249 is 0.4425809383392334 sec\n",
      "Time for epoch 250 is 0.36001038551330566 sec\n",
      "Time for epoch 251 is 0.5965259075164795 sec\n",
      "Time for epoch 252 is 0.7637035846710205 sec\n",
      "Time for epoch 253 is 0.6937623023986816 sec\n",
      "Time for epoch 254 is 0.45398759841918945 sec\n",
      "Time for epoch 255 is 0.7428028583526611 sec\n",
      "Time for epoch 256 is 0.7895586490631104 sec\n",
      "Time for epoch 257 is 0.8238275051116943 sec\n",
      "Time for epoch 258 is 0.6879336833953857 sec\n",
      "Time for epoch 259 is 0.6647961139678955 sec\n",
      "Time for epoch 260 is 0.47827601432800293 sec\n",
      "Time for epoch 261 is 0.2878880500793457 sec\n",
      "Time for epoch 262 is 0.6508896350860596 sec\n",
      "Time for epoch 263 is 0.6131904125213623 sec\n",
      "Time for epoch 264 is 0.7235116958618164 sec\n",
      "Time for epoch 265 is 0.5085775852203369 sec\n",
      "Time for epoch 266 is 0.8951385021209717 sec\n",
      "Time for epoch 267 is 0.8649060726165771 sec\n",
      "Time for epoch 268 is 0.6833047866821289 sec\n",
      "Time for epoch 269 is 0.5810067653656006 sec\n",
      "Time for epoch 270 is 1.1394948959350586 sec\n",
      "Time for epoch 271 is 0.7151250839233398 sec\n",
      "Time for epoch 272 is 0.9505205154418945 sec\n",
      "Time for epoch 273 is 0.6870293617248535 sec\n",
      "Time for epoch 274 is 1.0617074966430664 sec\n",
      "Time for epoch 275 is 0.8973190784454346 sec\n",
      "Time for epoch 276 is 0.7253024578094482 sec\n",
      "Time for epoch 277 is 0.6598231792449951 sec\n",
      "Time for epoch 278 is 0.6527099609375 sec\n",
      "Time for epoch 279 is 0.7426886558532715 sec\n",
      "Time for epoch 280 is 0.6293630599975586 sec\n",
      "Time for epoch 281 is 0.5278658866882324 sec\n",
      "Time for epoch 282 is 0.9717700481414795 sec\n",
      "Time for epoch 283 is 0.7737038135528564 sec\n",
      "Time for epoch 284 is 0.7268791198730469 sec\n",
      "Time for epoch 285 is 0.9467647075653076 sec\n",
      "Time for epoch 286 is 0.5680720806121826 sec\n",
      "Time for epoch 287 is 0.7218127250671387 sec\n",
      "Time for epoch 288 is 0.5677671432495117 sec\n",
      "Time for epoch 289 is 0.6243746280670166 sec\n",
      "Time for epoch 290 is 0.7694141864776611 sec\n",
      "Time for epoch 291 is 0.4349367618560791 sec\n",
      "Time for epoch 292 is 0.6793718338012695 sec\n",
      "Time for epoch 293 is 1.05708646774292 sec\n",
      "Time for epoch 294 is 0.7074506282806396 sec\n",
      "Time for epoch 295 is 0.7188723087310791 sec\n",
      "Time for epoch 296 is 0.5642886161804199 sec\n",
      "Time for epoch 297 is 0.7273054122924805 sec\n",
      "Time for epoch 298 is 0.7545783519744873 sec\n",
      "Time for epoch 299 is 0.6278584003448486 sec\n",
      "Time for epoch 300 is 1.2322332859039307 sec\n",
      "Time for epoch 301 is 0.7184457778930664 sec\n",
      "Time for epoch 302 is 0.8153541088104248 sec\n",
      "Time for epoch 303 is 0.4786489009857178 sec\n",
      "Time for epoch 304 is 0.6794872283935547 sec\n",
      "Time for epoch 305 is 0.5141220092773438 sec\n",
      "Time for epoch 306 is 0.9738223552703857 sec\n",
      "Time for epoch 307 is 0.8379383087158203 sec\n",
      "Time for epoch 308 is 0.7534637451171875 sec\n",
      "Time for epoch 309 is 0.9322714805603027 sec\n",
      "Time for epoch 310 is 0.9482951164245605 sec\n",
      "Time for epoch 311 is 0.8122203350067139 sec\n",
      "Time for epoch 312 is 0.5410170555114746 sec\n",
      "Time for epoch 313 is 0.47488999366760254 sec\n",
      "Time for epoch 314 is 0.475085973739624 sec\n",
      "Time for epoch 315 is 0.958507776260376 sec\n",
      "Time for epoch 316 is 0.4166545867919922 sec\n",
      "Time for epoch 317 is 0.7307653427124023 sec\n",
      "Time for epoch 318 is 0.7119138240814209 sec\n",
      "Time for epoch 319 is 0.6750500202178955 sec\n",
      "Time for epoch 320 is 0.4372241497039795 sec\n",
      "Time for epoch 321 is 0.7844226360321045 sec\n",
      "Time for epoch 322 is 0.6144912242889404 sec\n",
      "Time for epoch 323 is 0.7253098487854004 sec\n",
      "Time for epoch 324 is 0.7221901416778564 sec\n",
      "Time for epoch 325 is 0.5045444965362549 sec\n",
      "Time for epoch 326 is 0.8230819702148438 sec\n",
      "Time for epoch 327 is 0.3700423240661621 sec\n",
      "Time for epoch 328 is 0.8632700443267822 sec\n",
      "Time for epoch 329 is 0.5605864524841309 sec\n",
      "Time for epoch 330 is 0.7483525276184082 sec\n",
      "Time for epoch 331 is 0.9507322311401367 sec\n",
      "Time for epoch 332 is 1.1745519638061523 sec\n",
      "Time for epoch 333 is 0.9537603855133057 sec\n",
      "Time for epoch 334 is 0.5571300983428955 sec\n",
      "Time for epoch 335 is 0.6842718124389648 sec\n",
      "Time for epoch 336 is 0.780869722366333 sec\n",
      "Time for epoch 337 is 0.8328065872192383 sec\n",
      "Time for epoch 338 is 0.8717954158782959 sec\n",
      "Time for epoch 339 is 1.163109302520752 sec\n",
      "Time for epoch 340 is 0.950749397277832 sec\n",
      "Time for epoch 341 is 0.7499709129333496 sec\n",
      "Time for epoch 342 is 0.5612564086914062 sec\n",
      "Time for epoch 343 is 0.5817372798919678 sec\n",
      "Time for epoch 344 is 0.7025864124298096 sec\n",
      "Time for epoch 345 is 1.0189213752746582 sec\n",
      "Time for epoch 346 is 0.6678662300109863 sec\n",
      "Time for epoch 347 is 0.5503787994384766 sec\n",
      "Time for epoch 348 is 0.9723784923553467 sec\n",
      "Time for epoch 349 is 0.5314488410949707 sec\n",
      "Time for epoch 350 is 0.5892856121063232 sec\n",
      "Time for epoch 351 is 0.8390424251556396 sec\n",
      "Time for epoch 352 is 0.8772721290588379 sec\n",
      "Time for epoch 353 is 0.8080155849456787 sec\n",
      "Time for epoch 354 is 0.8025925159454346 sec\n",
      "Time for epoch 355 is 0.5254621505737305 sec\n",
      "Time for epoch 356 is 0.7486732006072998 sec\n",
      "Time for epoch 357 is 0.7124395370483398 sec\n",
      "Time for epoch 358 is 0.6038055419921875 sec\n",
      "Time for epoch 359 is 0.8549654483795166 sec\n",
      "Time for epoch 360 is 0.6194210052490234 sec\n",
      "Time for epoch 361 is 0.711341381072998 sec\n",
      "Time for epoch 362 is 0.9706690311431885 sec\n",
      "Time for epoch 363 is 0.8835678100585938 sec\n",
      "Time for epoch 364 is 0.9168343544006348 sec\n",
      "Time for epoch 365 is 0.964277982711792 sec\n",
      "Time for epoch 366 is 0.5970916748046875 sec\n",
      "Time for epoch 367 is 0.7256999015808105 sec\n",
      "Time for epoch 368 is 0.49999046325683594 sec\n",
      "Time for epoch 369 is 0.6102595329284668 sec\n",
      "Time for epoch 370 is 1.1035332679748535 sec\n",
      "Time for epoch 371 is 0.6208736896514893 sec\n",
      "Time for epoch 372 is 0.5691430568695068 sec\n",
      "Time for epoch 373 is 0.3806023597717285 sec\n",
      "Time for epoch 374 is 0.8191566467285156 sec\n",
      "Time for epoch 375 is 0.8893752098083496 sec\n",
      "Time for epoch 376 is 0.6876688003540039 sec\n",
      "Time for epoch 377 is 0.6387248039245605 sec\n",
      "Time for epoch 378 is 0.6144917011260986 sec\n",
      "Time for epoch 379 is 0.867917537689209 sec\n",
      "Time for epoch 380 is 0.7430269718170166 sec\n",
      "Time for epoch 381 is 0.8481080532073975 sec\n",
      "Time for epoch 382 is 0.9704527854919434 sec\n",
      "Time for epoch 383 is 0.7389473915100098 sec\n",
      "Time for epoch 384 is 0.9151182174682617 sec\n",
      "Time for epoch 385 is 0.5976147651672363 sec\n",
      "Time for epoch 386 is 0.5582308769226074 sec\n",
      "Time for epoch 387 is 0.410114049911499 sec\n",
      "Time for epoch 388 is 0.5754954814910889 sec\n",
      "Time for epoch 389 is 0.6610932350158691 sec\n",
      "Time for epoch 390 is 0.7881500720977783 sec\n",
      "Time for epoch 391 is 0.7862124443054199 sec\n",
      "Time for epoch 392 is 0.8899242877960205 sec\n",
      "Time for epoch 393 is 0.8199164867401123 sec\n",
      "Time for epoch 394 is 0.48848748207092285 sec\n",
      "Time for epoch 395 is 0.7457621097564697 sec\n",
      "Time for epoch 396 is 0.6302282810211182 sec\n",
      "Time for epoch 397 is 0.3671705722808838 sec\n",
      "Time for epoch 398 is 0.44420289993286133 sec\n",
      "Time for epoch 399 is 0.7753486633300781 sec\n",
      "Time for epoch 400 is 0.7456717491149902 sec\n",
      "Time for epoch 401 is 0.7922112941741943 sec\n",
      "Time for epoch 402 is 0.6596307754516602 sec\n",
      "Time for epoch 403 is 0.5704748630523682 sec\n",
      "Time for epoch 404 is 0.47560834884643555 sec\n",
      "Time for epoch 405 is 0.7784202098846436 sec\n",
      "Time for epoch 406 is 0.9762084484100342 sec\n",
      "Time for epoch 407 is 0.8956224918365479 sec\n",
      "Time for epoch 408 is 0.526940107345581 sec\n",
      "Time for epoch 409 is 0.8648531436920166 sec\n",
      "Time for epoch 410 is 1.0976452827453613 sec\n",
      "Time for epoch 411 is 0.6052365303039551 sec\n",
      "Time for epoch 412 is 0.5750715732574463 sec\n",
      "Time for epoch 413 is 0.7159945964813232 sec\n",
      "Time for epoch 414 is 0.34778666496276855 sec\n",
      "Time for epoch 415 is 0.4953649044036865 sec\n",
      "Time for epoch 416 is 0.4853515625 sec\n",
      "Time for epoch 417 is 0.4995591640472412 sec\n",
      "Time for epoch 418 is 0.5470888614654541 sec\n",
      "Time for epoch 419 is 0.7469427585601807 sec\n",
      "Time for epoch 420 is 0.7433404922485352 sec\n",
      "Time for epoch 421 is 0.9383513927459717 sec\n",
      "Time for epoch 422 is 0.4826183319091797 sec\n",
      "Time for epoch 423 is 0.763634443283081 sec\n",
      "Time for epoch 424 is 0.6484129428863525 sec\n",
      "Time for epoch 425 is 0.5456366539001465 sec\n",
      "Time for epoch 426 is 0.638117790222168 sec\n",
      "Time for epoch 427 is 0.4148073196411133 sec\n",
      "Time for epoch 428 is 0.6110007762908936 sec\n",
      "Time for epoch 429 is 0.6869168281555176 sec\n",
      "Time for epoch 430 is 0.3733038902282715 sec\n",
      "Time for epoch 431 is 0.6107773780822754 sec\n",
      "Time for epoch 432 is 0.5878233909606934 sec\n",
      "Time for epoch 433 is 0.30866456031799316 sec\n",
      "Time for epoch 434 is 0.30530261993408203 sec\n",
      "Time for epoch 435 is 0.707289457321167 sec\n",
      "Time for epoch 436 is 0.5964245796203613 sec\n",
      "Time for epoch 437 is 0.7801554203033447 sec\n",
      "Time for epoch 438 is 0.5242061614990234 sec\n",
      "Time for epoch 439 is 1.021245002746582 sec\n",
      "Time for epoch 440 is 0.7441771030426025 sec\n",
      "Time for epoch 441 is 0.46454739570617676 sec\n",
      "Time for epoch 442 is 0.4385976791381836 sec\n",
      "Time for epoch 443 is 0.8932979106903076 sec\n",
      "Time for epoch 444 is 0.47873997688293457 sec\n",
      "Time for epoch 445 is 0.4729757308959961 sec\n",
      "Time for epoch 446 is 0.6035804748535156 sec\n",
      "Time for epoch 447 is 0.7754268646240234 sec\n",
      "Time for epoch 448 is 0.440004825592041 sec\n",
      "Time for epoch 449 is 0.5871737003326416 sec\n",
      "Time for epoch 450 is 0.8473021984100342 sec\n",
      "Time for epoch 451 is 0.9831399917602539 sec\n",
      "Time for epoch 452 is 1.0830276012420654 sec\n",
      "Time for epoch 453 is 0.8866338729858398 sec\n",
      "Time for epoch 454 is 0.6362459659576416 sec\n",
      "Time for epoch 455 is 0.48702454566955566 sec\n",
      "Time for epoch 456 is 0.577268123626709 sec\n",
      "Time for epoch 457 is 0.5723264217376709 sec\n",
      "Time for epoch 458 is 0.3642234802246094 sec\n",
      "Time for epoch 459 is 0.5760190486907959 sec\n",
      "Time for epoch 460 is 0.6720354557037354 sec\n",
      "Time for epoch 461 is 0.9444694519042969 sec\n",
      "Time for epoch 462 is 0.7284719944000244 sec\n",
      "Time for epoch 463 is 0.6012477874755859 sec\n",
      "Time for epoch 464 is 0.4528193473815918 sec\n",
      "Time for epoch 465 is 0.7265081405639648 sec\n",
      "Time for epoch 466 is 0.9778056144714355 sec\n",
      "Time for epoch 467 is 0.7669339179992676 sec\n",
      "Time for epoch 468 is 0.8313512802124023 sec\n",
      "Time for epoch 469 is 0.8610842227935791 sec\n",
      "Time for epoch 470 is 0.5913493633270264 sec\n",
      "Time for epoch 471 is 0.4467446804046631 sec\n",
      "Time for epoch 472 is 0.671196460723877 sec\n",
      "Time for epoch 473 is 0.5825917720794678 sec\n",
      "Time for epoch 474 is 0.6183936595916748 sec\n",
      "Time for epoch 475 is 0.6866967678070068 sec\n",
      "Time for epoch 476 is 0.49639058113098145 sec\n",
      "Time for epoch 477 is 0.782717227935791 sec\n",
      "Time for epoch 478 is 0.47356677055358887 sec\n",
      "Time for epoch 479 is 0.5550656318664551 sec\n",
      "Time for epoch 480 is 0.9602718353271484 sec\n",
      "Time for epoch 481 is 0.4241299629211426 sec\n",
      "Time for epoch 482 is 0.6445293426513672 sec\n",
      "Time for epoch 483 is 0.45113515853881836 sec\n",
      "Time for epoch 484 is 0.5693082809448242 sec\n",
      "Time for epoch 485 is 0.7022817134857178 sec\n",
      "Time for epoch 486 is 0.5368399620056152 sec\n",
      "Time for epoch 487 is 0.6417357921600342 sec\n",
      "Time for epoch 488 is 0.6410741806030273 sec\n",
      "Time for epoch 489 is 0.6843669414520264 sec\n",
      "Time for epoch 490 is 0.5556771755218506 sec\n",
      "Time for epoch 491 is 0.41112327575683594 sec\n",
      "Time for epoch 492 is 0.6962885856628418 sec\n",
      "Time for epoch 493 is 0.6526296138763428 sec\n",
      "Time for epoch 494 is 0.41957902908325195 sec\n",
      "Time for epoch 495 is 0.652620792388916 sec\n",
      "Time for epoch 496 is 0.6973228454589844 sec\n",
      "Time for epoch 497 is 0.7604453563690186 sec\n",
      "Time for epoch 498 is 0.7259094715118408 sec\n",
      "Time for epoch 499 is 0.8833513259887695 sec\n",
      "Time for epoch 500 is 0.7903125286102295 sec\n"
     ]
    }
   ],
   "source": [
    "train(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 4ms/step\n",
      "[[0.5454503  0.45454973]\n",
      " [0.47031873 0.52968127]\n",
      " [0.54570436 0.45429558]\n",
      " ...\n",
      " [0.49318314 0.50681686]\n",
      " [0.5340632  0.46593684]\n",
      " [0.49426886 0.50573117]]\n",
      "1.0\n",
      "0.993\n",
      "0.139\n"
     ]
    }
   ],
   "source": [
    "noise = tf.random.normal([1000, 100])\n",
    "fake_seqs = generator(noise, training=False)\n",
    "\n",
    "prediction = discriminator.predict([fake_seqs[:,:15,:]])\n",
    "print(prediction)\n",
    "\n",
    "over50 = 0\n",
    "over25 = 0\n",
    "over40 = 0\n",
    "total = 0\n",
    "for key, output in enumerate(prediction):\n",
    "    if output[1] > 0.25:\n",
    "        over25 += 1\n",
    "    if output[1] > 0.4:\n",
    "        over40 += 1\n",
    "    if output[1] > 0.5:\n",
    "        over50 += 1\n",
    "    total += 1\n",
    "\n",
    "print(over25 / total)\n",
    "print(over40 / total)\n",
    "print(over50 / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 5ms/step\n",
      "round 1:a, over5%=1.0 over 25%=1.0, over 40%=1.0, over 45%=0.858, over 50%=0.027\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "round 2:a, over5%=1.0 over 25%=1.0, over 40%=1.0, over 45%=0.852, over 50%=0.025\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "round 3:a, over5%=1.0 over 25%=1.0, over 40%=1.0, over 45%=0.838, over 50%=0.029\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "round 4:a, over5%=1.0 over 25%=1.0, over 40%=1.0, over 45%=0.859, over 50%=0.023\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "round 5:a, over5%=1.0 over 25%=1.0, over 40%=1.0, over 45%=0.884, over 50%=0.029\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "round 6:a, over5%=1.0 over 25%=1.0, over 40%=1.0, over 45%=0.86, over 50%=0.039\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "round 7:a, over5%=1.0 over 25%=1.0, over 40%=1.0, over 45%=0.853, over 50%=0.022\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "round 8:a, over5%=1.0 over 25%=1.0, over 40%=1.0, over 45%=0.859, over 50%=0.027\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "round 9:a, over5%=1.0 over 25%=1.0, over 40%=1.0, over 45%=0.868, over 50%=0.033\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "round 10:a, over5%=1.0 over 25%=1.0, over 40%=1.0, over 45%=0.867, over 50%=0.021\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "round 11:a, over5%=1.0 over 25%=1.0, over 40%=1.0, over 45%=0.852, over 50%=0.017\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "round 12:a, over5%=1.0 over 25%=1.0, over 40%=1.0, over 45%=0.863, over 50%=0.032\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "round 13:a, over5%=1.0 over 25%=1.0, over 40%=1.0, over 45%=0.875, over 50%=0.029\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "round 14:a, over5%=1.0 over 25%=1.0, over 40%=1.0, over 45%=0.858, over 50%=0.029\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "round 15:a, over5%=1.0 over 25%=1.0, over 40%=1.0, over 45%=0.846, over 50%=0.032\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "round 16:a, over5%=1.0 over 25%=1.0, over 40%=1.0, over 45%=0.86, over 50%=0.036\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "round 17:a, over5%=1.0 over 25%=1.0, over 40%=1.0, over 45%=0.853, over 50%=0.028\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "round 18:a, over5%=1.0 over 25%=1.0, over 40%=1.0, over 45%=0.856, over 50%=0.043\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "round 19:a, over5%=1.0 over 25%=1.0, over 40%=1.0, over 45%=0.867, over 50%=0.031\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "round 20:a, over5%=1.0 over 25%=1.0, over 40%=1.0, over 45%=0.849, over 50%=0.03\n"
     ]
    }
   ],
   "source": [
    "rounds = 20\n",
    "for i in range(rounds):\n",
    "    noise = tf.random.normal([1000, 100])\n",
    "    fake_seqs = generator(noise, training=False)\n",
    "\n",
    "    prediction = discriminator.predict([fake_seqs[:,:15,:]])\n",
    "\n",
    "    over50 = 0\n",
    "    over25 = 0\n",
    "    over40 = 0\n",
    "    over45 = 0\n",
    "    over5 = 0\n",
    "    total = 0\n",
    "    for key, output in enumerate(prediction):\n",
    "        if output[1] > 0.25:\n",
    "            over25 += 1\n",
    "        if output[1] > 0.4:\n",
    "            over40 += 1\n",
    "        if output[1] > 0.45:\n",
    "            over45 += 1\n",
    "        if output[1] > 0.5:\n",
    "            over50 += 1\n",
    "        if output[1] > 0.05:\n",
    "            over5 += 1\n",
    "        total += 1\n",
    "\n",
    "    print(f\"round {i + 1}:a, over5%={over5 / total} over 25%={over25 / total}, over 40%={over40 / total}, over 45%={over45 / total}, over 50%={over50 / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
