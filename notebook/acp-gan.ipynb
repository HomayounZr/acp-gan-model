{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 23:02:10.618472: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.12.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(740, 15, 20)\n",
      "(740, 15, 31)\n",
      "(740, 15, 20)\n",
      "(740, 2)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical as labelEncoding \n",
    "\n",
    "T = 15 # terminus_length\n",
    "\n",
    "X1 = np.load('bpf-740.npy')\n",
    "X2 = np.load('bits-740.npy')\n",
    "X3 = np.load('blosum-740.npy')\n",
    "\n",
    "\n",
    "X1 = X1[:,0:T,:]\n",
    "X2 = X2[:,0:T,:]\n",
    "X3 = X3[:,0:T,:]\n",
    "\n",
    "\n",
    "Y  = [1 for _ in range(376)]\n",
    "Y += [0 for _ in range(364)]\n",
    "\n",
    "Y = labelEncoding(Y, dtype=int)\n",
    "\n",
    "print(X1.shape)\n",
    "print(X2.shape)\n",
    "print(X3.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(740, 15, 20)\n",
      "(15, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X1.shape)\n",
    "print(X1[0].shape)\n",
    "X1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(740, 15, 31)\n",
      "(15, 31)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 1, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 1, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 1, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 1, 0, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X2.shape)\n",
    "print(X2[0].shape)\n",
    "X2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(740, 15, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0, -3, -1, -2, -3,  6, -2, -4, -2, -4, -3,  0, -2, -2, -2,  0,\n",
       "        -2, -3, -2, -3],\n",
       "       [-1, -1, -4, -3,  0, -4, -3,  2, -2,  4,  2, -3, -3, -2, -2, -2,\n",
       "        -1,  1, -2, -1],\n",
       "       [-3, -2, -4, -3,  1, -2, -2, -3, -3, -2, -1, -4, -4, -2, -3, -3,\n",
       "        -2, -3, 11,  2],\n",
       "       [ 1, -1,  0,  0, -2,  0, -1, -2,  0, -2, -1,  1, -1,  0, -1,  4,\n",
       "         1, -2, -3, -2],\n",
       "       [-1, -3, -1,  1, -3, -2, -1, -3,  5, -2, -1,  0, -1,  1,  2,  0,\n",
       "        -1, -2, -3, -2],\n",
       "       [-1, -1, -3, -3,  0, -4, -3,  4, -3,  2,  1, -3, -3, -3, -3, -2,\n",
       "        -1,  3, -3, -1],\n",
       "       [-1, -3, -1,  1, -3, -2, -1, -3,  5, -2, -1,  0, -1,  1,  2,  0,\n",
       "        -1, -2, -3, -2],\n",
       "       [-1, -4,  2,  5, -3, -2,  0, -3,  1, -3, -2,  0, -1,  2,  0,  0,\n",
       "        -1, -2, -3, -2],\n",
       "       [ 0, -1, -3, -2, -1, -3, -3,  3, -2,  1,  1, -3, -2, -2, -3, -2,\n",
       "         0,  4, -3, -1],\n",
       "       [ 0, -3, -1, -2, -3,  6, -2, -4, -2, -4, -3,  0, -2, -2, -2,  0,\n",
       "        -2, -3, -2, -3],\n",
       "       [-1, -3, -1,  1, -3, -2, -1, -3,  5, -2, -1,  0, -1,  1,  2,  0,\n",
       "        -1, -2, -3, -2],\n",
       "       [-1, -4,  2,  5, -3, -2,  0, -3,  1, -3, -2,  0, -1,  2,  0,  0,\n",
       "        -1, -2, -3, -2],\n",
       "       [ 4,  0, -2, -1, -2,  0, -2, -1, -1, -1, -1, -2, -1, -1, -1,  1,\n",
       "         0,  0, -3, -2],\n",
       "       [ 4,  0, -2, -1, -2,  0, -2, -1, -1, -1, -1, -2, -1, -1, -1,  1,\n",
       "         0,  0, -3, -2],\n",
       "       [-1, -3, -1,  1, -3, -2, -1, -3,  5, -2, -1,  0, -1,  1,  2,  0,\n",
       "        -1, -2, -3, -2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X3.shape)\n",
    "X3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're using TF-2.12.0.\n"
     ]
    }
   ],
   "source": [
    "# Deep Neural Networks:\n",
    "import tensorflow as tf; print('We\\'re using TF-{}.'.format(tf.__version__))\n",
    "# import keras; print('We\\'re using Keras-{}.'.format(keras.__version__))\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, Flatten, BatchNormalization,\n",
    "                                     Conv1D, Conv2D, MaxPooling1D, MaxPooling2D,\n",
    "                                     LSTM, GRU, Embedding, Bidirectional, Concatenate)\n",
    "from tensorflow.keras.regularizers import (l1, l2, l1_l2)\n",
    "from tensorflow.keras.optimizers import (RMSprop, Adam, SGD)\n",
    "from tensorflow.keras.models import (Sequential, Model)\n",
    "\n",
    "# Core:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Performance:\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, matthews_corrcoef, precision_score, roc_curve, auc)\n",
    "from sklearn.model_selection import (StratifiedKFold, KFold, train_test_split)\n",
    "\n",
    "#Utilities:\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical as labelEncoding   # Usages: Y = labelEncoding(Y, dtype=int)\n",
    "from tensorflow.keras.utils import plot_model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    ### Head-1:\n",
    "    input1 = Input(shape=X1[0].shape)\n",
    "\n",
    "    x = Conv1D(filters=10, kernel_size=4, padding='same', activation='relu', kernel_regularizer=l2(l=0.01))(input1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.80)(x)\n",
    "\n",
    "    x = Conv1D(filters=8, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(l=0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.70)(x)\n",
    "\n",
    "    head1 = Flatten()(x)\n",
    "\n",
    "\n",
    "    ### Head-2:\n",
    "    # input2 = Input(shape=X2[0].shape)\n",
    "\n",
    "    # x = Conv1D(filters=10, kernel_size=4, padding='same', activation='relu', kernel_regularizer=l2(l=0.01))(input2)\n",
    "    # x = BatchNormalization()(x)\n",
    "    # x = Dropout(rate=0.70)(x)\n",
    "\n",
    "    # x = Conv1D(filters=8, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(l=0.01))(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    # x = Dropout(rate=0.70)(x)\n",
    "\n",
    "    # head2 = Flatten()(x)\n",
    "\n",
    "\n",
    "    ### Head-3:\n",
    "    input3 = Input(shape=X3[0].shape)\n",
    "\n",
    "    x = Conv1D(filters=10, kernel_size=4, padding='same', activation='relu',)(input3)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.70)(x)\n",
    "\n",
    "    x = Conv1D(filters=8, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(l=0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.70)(x)\n",
    "\n",
    "    head3 = Flatten()(x)\n",
    "\n",
    "\n",
    "    # merge\n",
    "    merge = Concatenate()([head1, head3])\n",
    "\n",
    "    output = Dense(units=8, activation='relu', kernel_regularizer=l2(l=0.01))(merge)\n",
    "    output = BatchNormalization()(output)\n",
    "    output = Dropout(rate=0.70)(output)\n",
    "\n",
    "    output = Dense(units=2, activation='softmax')(output)\n",
    "\n",
    "    return Model(inputs=[input1, input3], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "from Bio.SeqUtils import ProtParam\n",
    "from Bio.Align import substitution_matrices\n",
    "from Bio.SubsMat import MatrixInfo\n",
    "\n",
    "\n",
    "# Function to convert binary profile feature to amino acid sequence\n",
    "def bpf_to_sequence(binary_profile_feature):\n",
    "    amino_acids = \"ARNDCQEGHILKMFPSTWYVBZX\"\n",
    "    sequence = \"\"\n",
    "    for row in binary_profile_feature:\n",
    "        index = row.argmax()\n",
    "        sequence += amino_acids[index]\n",
    "    return sequence\n",
    "\n",
    "def seq_to_blosum(sequence):\n",
    "    blosum62 = MatrixInfo.blosum62\n",
    "    sequence = sequence.upper()\n",
    "    length = len(sequence)\n",
    "    blosum_matrix = []\n",
    "    for aa1 in sequence:\n",
    "        row = []\n",
    "        for aa2 in sequence:\n",
    "            if (aa1, aa2) in blosum62:\n",
    "                row.append(blosum62[(aa1, aa2)])\n",
    "            else:\n",
    "                row.append(blosum62[(aa2, aa1)])\n",
    "        blosum_matrix.append(row)\n",
    "    return blosum_matrix\n",
    "# blosum62 = MatrixInfo.blosum62\n",
    "\n",
    "# def seq_to_blosum(sequence):\n",
    "#     sequence = tf.strings.upper(sequence)  # Ensure sequence is uppercase\n",
    "#     sequence_length = tf.shape(sequence)[0]\n",
    "#     blosum_matrix = []\n",
    "    \n",
    "#     # Iterate over sequence elements\n",
    "#     for i in range(sequence_length):\n",
    "#         row = []\n",
    "#         aa1 = sequence[i]\n",
    "#         # Iterate over sequence elements again to compare with aa1\n",
    "#         for j in range(sequence_length):\n",
    "#             aa2 = sequence[j]\n",
    "#             # Check if (aa1, aa2) or (aa2, aa1) is in the Blosum matrix\n",
    "#             if (aa1, aa2) in blosum62:\n",
    "#                 row.append(blosum62[(aa1, aa2)])\n",
    "#             else:\n",
    "#                 row.append(blosum62[(aa2, aa1)])\n",
    "#         blosum_matrix.append(row)\n",
    "    \n",
    "#     print(blosum_matrix)\n",
    "#     return tf.convert_to_tensor(np.array(blosum_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dim, output_shape):\n",
    "    model = Sequential([\n",
    "        layers.Input(shape=(latent_dim,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(output_shape[0] * output_shape[1], activation='sigmoid'),\n",
    "        layers.Reshape(output_shape),\n",
    "        layers.Lambda(lambda x: tf.one_hot(tf.argmax(x, axis=-1), depth=output_shape[1])),\n",
    "        layers.Lambda(lambda x: tf.cast(x, dtype=tf.int32))\n",
    "    ])\n",
    "    return model\n",
    "    # inputs = layers.Input(shape=(latent_dim,))\n",
    "    # x = layers.Dense(128, activation='relu')(inputs)\n",
    "    # x = layers.Dense(256, activation='relu')(x)\n",
    "    # x = layers.Dense(output_shape[0] * output_shape[1], activation='sigmoid')(x)\n",
    "    # x = layers.Reshape(output_shape)(x)\n",
    "    # x = layers.Lambda(lambda x: tf.one_hot(tf.argmax(x, axis=-1), depth=output_shape[1]))(x)\n",
    "    # x = layers.Lambda(lambda x: tf.cast(x, dtype=tf.int32))(x)\n",
    "    \n",
    "    # # Define the custom layer\n",
    "    # class CustomLayer(layers.Layer):\n",
    "    #     def call(self, inputs):\n",
    "    #         sequence = tf.keras.backend.eval(tf.argmax(inputs, axis=-1))  # Extract sequence from one-hot encoding\n",
    "    #         blosum_matrix = seq_to_blosum(sequence)  # Generate Blosum matrix\n",
    "    #         print(blosum_matrix)\n",
    "    #         return tf.convert_to_tensor(np.array(blosum_matrix))  # Convert Blosum matrix to tensor\n",
    "    \n",
    "    # custom_output = CustomLayer()(x)\n",
    "    \n",
    "    # # Combine the generator output and custom output\n",
    "    # combined_outputs = layers.Concatenate()([x, custom_output])\n",
    "    \n",
    "    # # Define the model\n",
    "    # model = Model(inputs=inputs, outputs=[x, custom_output])\n",
    "    \n",
    "    # return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions\n",
    "latent_dim = 100  # Dimensionality of the latent space\n",
    "output_shape = (25, 20)  # Desired output shape of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 25, 20), dtype=int32, numpy=\n",
       "array([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = build_generator(latent_dim, output_shape)\n",
    "\n",
    "noise = tf.random.normal([1, latent_dim])\n",
    "\n",
    "generated_matrix = generator(noise, training=False)\n",
    "\n",
    "generated_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Profile Feature Matrix:\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "Amino Acid Sequence:\n",
      "tf.Tensor(b'LNPLDHFQQHTMHINAVSIGFATFA', shape=(), dtype=string)\n",
      "\n",
      "BLOSUM62 Matrix:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'upper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# blosum62 = substitution_matrices.load(\"BLOSUM62\")\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLOSUM62 Matrix:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m fake_blosum \u001b[38;5;241m=\u001b[39m seq_to_blosum(sequence)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(fake_blosum)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Calculate physiochemical properties\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# protein_seq = Seq(sequence)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# protein_analyzer = ProtParam.ProteinAnalysis(str(protein_seq))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# physiochemical_matrix = protein_analyzer.protein_scale(window=5)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Display physiochemical properties matrix\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[45], line 18\u001b[0m, in \u001b[0;36mseq_to_blosum\u001b[0;34m(sequence)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mseq_to_blosum\u001b[39m(sequence):\n\u001b[1;32m     17\u001b[0m     blosum62 \u001b[38;5;241m=\u001b[39m MatrixInfo\u001b[38;5;241m.\u001b[39mblosum62\n\u001b[0;32m---> 18\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m sequence\u001b[38;5;241m.\u001b[39mupper()\n\u001b[1;32m     19\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sequence)\n\u001b[1;32m     20\u001b[0m     blosum_matrix \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/acp-proj/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:443\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mravel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranspose\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    435\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtolist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    436\u001b[0m   \u001b[38;5;66;03m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001b[39;00m\n\u001b[1;32m    437\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    438\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;124m    from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124m    np_config.enable_numpy_behavior()\u001b[39m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m--> 443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'upper'"
     ]
    }
   ],
   "source": [
    "fake_bpf = generated_matrix[0].numpy()\n",
    "\n",
    "print(\"Binary Profile Feature Matrix:\")\n",
    "print(fake_bpf.tolist())\n",
    "print()\n",
    "\n",
    "# Convert binary profile feature to amino acid sequence\n",
    "sequence = bpf_to_sequence(generated_matrix[0].numpy())\n",
    "print(\"Amino Acid Sequence:\")\n",
    "print(sequence)\n",
    "print()\n",
    "\n",
    "# blosum62 = substitution_matrices.load(\"BLOSUM62\")\n",
    "\n",
    "print(\"BLOSUM62 Matrix:\")\n",
    "fake_blosum = seq_to_blosum(sequence)\n",
    "print(fake_blosum)\n",
    "\n",
    "# Calculate physiochemical properties\n",
    "# protein_seq = Seq(sequence)\n",
    "# protein_analyzer = ProtParam.ProteinAnalysis(str(protein_seq))\n",
    "# physiochemical_matrix = protein_analyzer.protein_scale(window=5)\n",
    "\n",
    "# Display physiochemical properties matrix\n",
    "print(\"Physiochemical Properties Matrix:\")\n",
    "# print(physiochemical_matrix)\n",
    "\n",
    "def convert_input(generated_seq):\n",
    "    fake_bpf = np.array(generated_seq)\n",
    "    seq = bpf_to_sequence(fake_bpf)\n",
    "    return np.array(fake_bpf), np.array(seq_to_blosum(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_discriminator()\n",
    "discriminator.load_weights('./acp_mhcnn_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 242ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.10887526, 0.8911247 ],\n",
       "       [0.21293242, 0.7870676 ],\n",
       "       [0.1744912 , 0.8255088 ],\n",
       "       [0.17307754, 0.82692254],\n",
       "       [0.13898307, 0.8610169 ]], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = discriminator.predict([X1[:5,:,:], X3[:5,:,:]])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'upper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m inputs_blosum \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m fake_seqs:\n\u001b[0;32m----> 7\u001b[0m     bpf, blosum \u001b[38;5;241m=\u001b[39m convert_input(seq)\n\u001b[1;32m      8\u001b[0m     inputs_bpf\u001b[38;5;241m.\u001b[39mappend(bpf[:\u001b[38;5;241m15\u001b[39m])\n\u001b[1;32m      9\u001b[0m     inputs_blosum\u001b[38;5;241m.\u001b[39mappend(blosum[:\u001b[38;5;241m15\u001b[39m,:\u001b[38;5;241m20\u001b[39m])\n",
      "Cell \u001b[0;32mIn[49], line 31\u001b[0m, in \u001b[0;36mconvert_input\u001b[0;34m(generated_seq)\u001b[0m\n\u001b[1;32m     29\u001b[0m fake_bpf \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(generated_seq)\n\u001b[1;32m     30\u001b[0m seq \u001b[38;5;241m=\u001b[39m bpf_to_sequence(fake_bpf)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(fake_bpf), np\u001b[38;5;241m.\u001b[39marray(seq_to_blosum(seq))\n",
      "Cell \u001b[0;32mIn[45], line 18\u001b[0m, in \u001b[0;36mseq_to_blosum\u001b[0;34m(sequence)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mseq_to_blosum\u001b[39m(sequence):\n\u001b[1;32m     17\u001b[0m     blosum62 \u001b[38;5;241m=\u001b[39m MatrixInfo\u001b[38;5;241m.\u001b[39mblosum62\n\u001b[0;32m---> 18\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m sequence\u001b[38;5;241m.\u001b[39mupper()\n\u001b[1;32m     19\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sequence)\n\u001b[1;32m     20\u001b[0m     blosum_matrix \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/acp-proj/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:443\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mravel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranspose\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    435\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtolist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    436\u001b[0m   \u001b[38;5;66;03m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001b[39;00m\n\u001b[1;32m    437\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    438\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;124m    from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124m    np_config.enable_numpy_behavior()\u001b[39m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m--> 443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'upper'"
     ]
    }
   ],
   "source": [
    "noise = tf.random.normal([5, 100])\n",
    "fake_seqs = generator(noise, training=False)\n",
    "\n",
    "inputs_bpf = []\n",
    "inputs_blosum = []\n",
    "for seq in fake_seqs:\n",
    "    bpf, blosum = convert_input(seq)\n",
    "    inputs_bpf.append(bpf[:15])\n",
    "    inputs_blosum.append(blosum[:15,:20])\n",
    "\n",
    "inputs_bpf = np.array(inputs_bpf)\n",
    "inputs_blosum = np.array(inputs_blosum)\n",
    "\n",
    "# print(X3[:5,:,:].shape)\n",
    "# print(X3[0])\n",
    "# print('---------------')\n",
    "# print(inputs_blosum.shape)\n",
    "# print(inputs_blosum[0])\n",
    "\n",
    "prediction = discriminator.predict([inputs_bpf, inputs_blosum])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(disc_output):\n",
    "    # print(disc_output.shape)\n",
    "    desired_output = np.zeros_like(disc_output)\n",
    "    desired_output[:, 1] = 1\n",
    "    return cross_entropy(desired_output, disc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"sequential_1/lambda_21/Cast:0\", shape=(5, 25, 20), dtype=int32)\n",
      "Tensor(\"ReduceJoin/ReduceJoin:0\", shape=(5,), dtype=string)\n",
      "Tensor(\"map/while/TensorArrayV2Read/TensorListGetItem:0\", shape=(25, 20), dtype=int32)\n",
      "Tensor(\"PartitionedCall:0\", shape=(5, 25, 20), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# tf.compat.v1.enable_eager_execution()\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "def bpf_to_sequence(binary_profile_feature):\n",
    "    amino_acids = \"ARNDCQEGHILKMFPSTWYVBZX\"\n",
    "    sequence_indices = tf.argmax(binary_profile_feature, axis=-1)  # Get the index of the maximum value in each row\n",
    "    sequence = tf.gather(tf.constant(list(amino_acids)), sequence_indices)  # Map indices to amino acids\n",
    "    sequence = tf.strings.reduce_join(sequence, axis=-1)  # Join amino acids to form the sequence\n",
    "    return sequence\n",
    "\n",
    "@tf.function\n",
    "def iterate_through_tensor(tensor):\n",
    "    def body(x):\n",
    "        # Do something with each element of the tensor\n",
    "        print(x)\n",
    "        return x  *2\n",
    "    \n",
    "    result = tf.map_fn(body, tensor)\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "@tf.function\n",
    "def test_tensor():\n",
    "    input_tensor = generator(noise, training=False)\n",
    "    print(input_tensor)\n",
    "    seq = bpf_to_sequence(input_tensor)\n",
    "    print(seq)\n",
    "    output_tensor = iterate_through_tensor(input_tensor)\n",
    "    print(output_tensor)\n",
    "\n",
    "test_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "blosum62 = MatrixInfo.blosum62\n",
    "\n",
    "\n",
    "def bpf_to_seq2(bpf):\n",
    "    amino_acids = \"ARNDCQEGHILKMFPSTWYVBZX\"\n",
    "    sequence_indices = tf.argmax(bpf, axis=-1)  # Get the index of the maximum value in each row\n",
    "    sequence = tf.gather(tf.constant(list(amino_acids)), sequence_indices)  # Map indices to amino acids\n",
    "    print(sequence)\n",
    "    for i in range(len(sequence)):\n",
    "        row = []\n",
    "        for j in range(len(sequence)):\n",
    "            a  = sequence[i]\n",
    "            b = sequence[j]\n",
    "            print(a)\n",
    "            print(b)\n",
    "            if (a, b) in blosum62:\n",
    "                row.append(blosum62[(a, b)])zip\n",
    "\n",
    "    return sequence\n",
    "\n",
    "@tf.function\n",
    "def seq_to_blosum2(sequence):\n",
    "    blosum62 = MatrixInfo.blosum62\n",
    "    blosum_matrix = []\n",
    "    for aa1 in sequence:\n",
    "        row = []\n",
    "        for aa2 in sequence:\n",
    "            if (aa1, aa2) in blosum62:\n",
    "                row.append(blosum62[(aa1, aa2)])\n",
    "            else:\n",
    "                row.append(blosum62[(aa2, aa1)])\n",
    "        blosum_matrix.append(row)\n",
    "    return blosum_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step():\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        fake_seqs_sym = generator(noise, training=True)\n",
    "        \n",
    "        inputs_bpf = []\n",
    "        for i in range(0, len(fake_seqs_sym)):\n",
    "            bpf = fake_seqs_sym[i]\n",
    "            print('kir')\n",
    "            seq = bpf_to_seq2(bpf)\n",
    "            print('kir')\n",
    "            inputs_bpf.append(bpf[:15])\n",
    "        \n",
    "        print(inputs_bpf)\n",
    "\n",
    "        inputs_blosum = []\n",
    "        # for seq in fake_seqs_sym:\n",
    "        #     bpf, blosum = convert_input(seq)\n",
    "        #     print(bpf)\n",
    "        #     print(blosum)\n",
    "        #     inputs_bpf.append(bpf[:15])\n",
    "        #     inputs_blosum.append(blosum[:15,:20])\n",
    "\n",
    "\n",
    "        fake_output = discriminator.predict([inputs_bpf, inputs_blosum])\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        print('loss', gen_loss)\n",
    "            \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    for gradient in gradients_of_generator:\n",
    "        print(\"Gradient:\", gradient)\n",
    "    \n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "def train(dataset = None, epochs = 50):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for batch in range(100):\n",
    "            # train_step(batch[0], batch[1])\n",
    "            train_step()\n",
    "\n",
    "        # Generate and save some examples of generated sequences\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            noise = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "            generated_sequences = generator(noise, training=False)\n",
    "            print(\"Examples of generated sequences at epoch\", epoch + 1)\n",
    "            for i in range(num_examples_to_generate):\n",
    "                print(bpf_to_sequence(generated_sequences[i].numpy()))\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kir\n",
      "Tensor(\"GatherV2:0\", shape=(25,), dtype=string)\n",
      "Tensor(\"strided_slice_1:0\", shape=(), dtype=string)\n",
      "Tensor(\"strided_slice_2:0\", shape=(), dtype=string)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_17154/1695240435.py\", line 12, in train_step  *\n        seq = bpf_to_seq2(bpf)\n    File \"/tmp/ipykernel_17154/2143957139.py\", line 16, in bpf_to_seq2  *\n        if (a, b) in blosum62:\n\n    TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[179], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Combine datasets X1 and X3\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# combined_dataset = tf.data.Dataset.from_tensor_slices((X1, X3)).shuffle(len(X1)).batch(BATCH_SIZE)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model for 50 epochs\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "Cell \u001b[0;32mIn[178], line 43\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m     39\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# train_step(batch[0], batch[1])\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     train_step()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Generate and save some examples of generated sequences\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/acp-proj/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileulnx6ajl.py:29\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m i \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m bpf \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbpf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m ag__\u001b[38;5;241m.\u001b[39mfor_stmt(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mrange\u001b[39m), (\u001b[38;5;241m0\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mlen\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(fake_seqs_sym),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), \u001b[38;5;28;01mNone\u001b[39;00m, loop_body, get_state, set_state, (), {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterate_names\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     30\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(ag__\u001b[38;5;241m.\u001b[39mld(inputs_bpf))\n\u001b[1;32m     31\u001b[0m inputs_blosum \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileulnx6ajl.py:23\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     21\u001b[0m bpf \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(fake_seqs_sym)[ag__\u001b[38;5;241m.\u001b[39mld(i)]\n\u001b[1;32m     22\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkir\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m seq \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(bpf_to_seq2), (ag__\u001b[38;5;241m.\u001b[39mld(bpf),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     24\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkir\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(inputs_bpf)\u001b[38;5;241m.\u001b[39mappend, (ag__\u001b[38;5;241m.\u001b[39mld(bpf)[:\u001b[38;5;241m15\u001b[39m],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file4etpkp_k.py:56\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__bpf_to_seq2\u001b[0;34m(bpf)\u001b[0m\n\u001b[1;32m     54\u001b[0m i \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     55\u001b[0m a \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m ag__\u001b[38;5;241m.\u001b[39mfor_stmt(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mrange\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mlen\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(sequence),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), \u001b[38;5;28;01mNone\u001b[39;00m, loop_body_1, get_state_2, set_state_2, (), {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterate_names\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file4etpkp_k.py:50\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__bpf_to_seq2.<locals>.loop_body_1\u001b[0;34m(itr_1)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mif_stmt((ag__\u001b[38;5;241m.\u001b[39mld(a), ag__\u001b[38;5;241m.\u001b[39mld(b)) \u001b[38;5;129;01min\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(blosum62), if_body, else_body, get_state, set_state, (), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m ag__\u001b[38;5;241m.\u001b[39mfor_stmt(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mrange\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mlen\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(sequence),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), \u001b[38;5;28;01mNone\u001b[39;00m, loop_body, get_state_1, set_state_1, (), {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterate_names\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mj\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file4etpkp_k.py:49\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__bpf_to_seq2.<locals>.loop_body_1.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melse_body\u001b[39m():\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt((ag__\u001b[38;5;241m.\u001b[39mld(a), ag__\u001b[38;5;241m.\u001b[39mld(b)) \u001b[38;5;129;01min\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(blosum62), if_body, else_body, get_state, set_state, (), \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_17154/1695240435.py\", line 12, in train_step  *\n        seq = bpf_to_seq2(bpf)\n    File \"/tmp/ipykernel_17154/2143957139.py\", line 16, in bpf_to_seq2  *\n        if (a, b) in blosum62:\n\n    TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.\n"
     ]
    }
   ],
   "source": [
    "# Combine datasets X1 and X3\n",
    "# combined_dataset = tf.data.Dataset.from_tensor_slices((X1, X3)).shuffle(len(X1)).batch(BATCH_SIZE)\n",
    "\n",
    "# Train the model for 50 epochs\n",
    "train(epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
