{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 20:58:26.295860: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're using TF-2.11.0.\n",
      "(740, 15, 20)\n",
      "(740, 15, 31)\n",
      "(740, 15, 20)\n",
      "(740, 2)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 15, 20)]     0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 15, 20)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 15, 10)       810         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 15, 10)       810         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 15, 10)      40          ['conv1d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 15, 10)      40          ['conv1d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 15, 10)       0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 15, 10)       0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 15, 8)        248         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 15, 8)        248         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 15, 8)       32          ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 15, 8)       32          ['conv1d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 15, 8)        0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 15, 8)        0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 120)          0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 120)          0           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 240)          0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 8)            1928        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 8)           32          ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 8)            0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2)            18          ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,238\n",
      "Trainable params: 4,150\n",
      "Non-trainable params: 88\n",
      "__________________________________________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 20:58:28.117219: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-27 20:58:28.129350: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "74/74 [==============================] - 5s 52ms/step - loss: 1.8800 - accuracy: 0.4865 - val_loss: 1.1585 - val_accuracy: 0.5135\n",
      "Epoch 2/500\n",
      "74/74 [==============================] - 3s 35ms/step - loss: 1.5411 - accuracy: 0.5270 - val_loss: 1.1767 - val_accuracy: 0.5203\n",
      "Epoch 3/500\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 1.4942 - accuracy: 0.5169 - val_loss: 1.1654 - val_accuracy: 0.5203\n",
      "Epoch 4/500\n",
      "74/74 [==============================] - 4s 48ms/step - loss: 1.3989 - accuracy: 0.5152 - val_loss: 1.1451 - val_accuracy: 0.5203\n",
      "Epoch 5/500\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 1.3335 - accuracy: 0.5101 - val_loss: 1.1205 - val_accuracy: 0.5203\n",
      "Epoch 6/500\n",
      "74/74 [==============================] - 4s 53ms/step - loss: 1.2276 - accuracy: 0.5034 - val_loss: 1.1024 - val_accuracy: 0.5270\n",
      "Epoch 7/500\n",
      "74/74 [==============================] - 4s 51ms/step - loss: 1.1948 - accuracy: 0.5304 - val_loss: 1.0887 - val_accuracy: 0.5203\n",
      "Epoch 8/500\n",
      "74/74 [==============================] - 3s 38ms/step - loss: 1.2193 - accuracy: 0.5118 - val_loss: 1.0771 - val_accuracy: 0.5203\n",
      "Epoch 9/500\n",
      "74/74 [==============================] - 3s 44ms/step - loss: 1.1147 - accuracy: 0.5507 - val_loss: 1.0665 - val_accuracy: 0.5203\n",
      "Epoch 10/500\n",
      "74/74 [==============================] - 3s 41ms/step - loss: 1.1230 - accuracy: 0.5034 - val_loss: 1.0551 - val_accuracy: 0.5203\n",
      "Epoch 11/500\n",
      "74/74 [==============================] - 3s 39ms/step - loss: 1.1065 - accuracy: 0.5000 - val_loss: 1.0463 - val_accuracy: 0.5203\n",
      "Epoch 12/500\n",
      "74/74 [==============================] - 2s 34ms/step - loss: 1.0758 - accuracy: 0.5338 - val_loss: 1.0382 - val_accuracy: 0.5203\n",
      "Epoch 13/500\n",
      "74/74 [==============================] - 4s 49ms/step - loss: 1.0555 - accuracy: 0.5574 - val_loss: 1.0274 - val_accuracy: 0.5203\n",
      "Epoch 14/500\n",
      "74/74 [==============================] - 3s 46ms/step - loss: 1.0395 - accuracy: 0.5355 - val_loss: 1.0179 - val_accuracy: 0.5203\n",
      "Epoch 15/500\n",
      "74/74 [==============================] - 4s 53ms/step - loss: 1.0159 - accuracy: 0.5608 - val_loss: 1.0115 - val_accuracy: 0.5203\n",
      "Epoch 16/500\n",
      "74/74 [==============================] - 4s 49ms/step - loss: 1.0007 - accuracy: 0.5845 - val_loss: 1.0046 - val_accuracy: 0.5203\n",
      "Epoch 17/500\n",
      "74/74 [==============================] - 3s 44ms/step - loss: 1.0032 - accuracy: 0.5355 - val_loss: 0.9947 - val_accuracy: 0.5203\n",
      "Epoch 18/500\n",
      "74/74 [==============================] - 3s 40ms/step - loss: 0.9921 - accuracy: 0.5659 - val_loss: 0.9867 - val_accuracy: 0.5203\n",
      "Epoch 19/500\n",
      "74/74 [==============================] - 4s 50ms/step - loss: 0.9802 - accuracy: 0.5557 - val_loss: 0.9755 - val_accuracy: 0.5135\n",
      "Epoch 20/500\n",
      "74/74 [==============================] - 4s 57ms/step - loss: 0.9728 - accuracy: 0.5236 - val_loss: 0.9648 - val_accuracy: 0.5270\n",
      "Epoch 21/500\n",
      "74/74 [==============================] - 4s 51ms/step - loss: 0.9586 - accuracy: 0.5794 - val_loss: 0.9551 - val_accuracy: 0.5203\n",
      "Epoch 22/500\n",
      "74/74 [==============================] - 4s 58ms/step - loss: 0.9326 - accuracy: 0.5507 - val_loss: 0.9466 - val_accuracy: 0.5203\n",
      "Epoch 23/500\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.9350 - accuracy: 0.5541 - val_loss: 0.9380 - val_accuracy: 0.5135\n",
      "Epoch 24/500\n",
      "74/74 [==============================] - 3s 40ms/step - loss: 0.9247 - accuracy: 0.5608 - val_loss: 0.9295 - val_accuracy: 0.5203\n",
      "Epoch 25/500\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.9190 - accuracy: 0.5828 - val_loss: 0.9173 - val_accuracy: 0.5405\n",
      "Epoch 26/500\n",
      "74/74 [==============================] - 4s 51ms/step - loss: 0.9086 - accuracy: 0.5659 - val_loss: 0.9047 - val_accuracy: 0.5405\n",
      "Epoch 27/500\n",
      "74/74 [==============================] - 3s 47ms/step - loss: 0.8851 - accuracy: 0.5861 - val_loss: 0.8940 - val_accuracy: 0.5541\n",
      "Epoch 28/500\n",
      "74/74 [==============================] - 4s 51ms/step - loss: 0.8775 - accuracy: 0.5845 - val_loss: 0.8830 - val_accuracy: 0.5608\n",
      "Epoch 29/500\n",
      "74/74 [==============================] - 3s 46ms/step - loss: 0.8722 - accuracy: 0.5997 - val_loss: 0.8718 - val_accuracy: 0.5541\n",
      "Epoch 30/500\n",
      "74/74 [==============================] - 3s 40ms/step - loss: 0.8562 - accuracy: 0.5895 - val_loss: 0.8605 - val_accuracy: 0.5608\n",
      "Epoch 31/500\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.8520 - accuracy: 0.6030 - val_loss: 0.8549 - val_accuracy: 0.5676\n",
      "Epoch 32/500\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.8464 - accuracy: 0.5895 - val_loss: 0.8518 - val_accuracy: 0.5676\n",
      "Epoch 33/500\n",
      "74/74 [==============================] - 3s 36ms/step - loss: 0.8312 - accuracy: 0.5963 - val_loss: 0.8404 - val_accuracy: 0.5676\n",
      "Epoch 34/500\n",
      "74/74 [==============================] - 4s 61ms/step - loss: 0.8274 - accuracy: 0.5929 - val_loss: 0.8309 - val_accuracy: 0.5608\n",
      "Epoch 35/500\n",
      "74/74 [==============================] - 3s 46ms/step - loss: 0.8177 - accuracy: 0.5811 - val_loss: 0.8201 - val_accuracy: 0.5743\n",
      "Epoch 36/500\n",
      "74/74 [==============================] - 4s 48ms/step - loss: 0.8149 - accuracy: 0.5878 - val_loss: 0.8157 - val_accuracy: 0.5608\n",
      "Epoch 37/500\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.7970 - accuracy: 0.6115 - val_loss: 0.8086 - val_accuracy: 0.5811\n",
      "Epoch 38/500\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.8014 - accuracy: 0.6166 - val_loss: 0.7999 - val_accuracy: 0.6014\n",
      "Epoch 39/500\n",
      "74/74 [==============================] - 3s 41ms/step - loss: 0.7744 - accuracy: 0.6318 - val_loss: 0.7876 - val_accuracy: 0.6014\n",
      "Epoch 40/500\n",
      "74/74 [==============================] - 3s 39ms/step - loss: 0.7712 - accuracy: 0.6351 - val_loss: 0.7744 - val_accuracy: 0.6216\n",
      "Epoch 41/500\n",
      "74/74 [==============================] - 2s 30ms/step - loss: 0.7744 - accuracy: 0.6115 - val_loss: 0.7651 - val_accuracy: 0.6351\n",
      "Epoch 42/500\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 0.7407 - accuracy: 0.6554 - val_loss: 0.7563 - val_accuracy: 0.6419\n",
      "Epoch 43/500\n",
      "74/74 [==============================] - 4s 48ms/step - loss: 0.7342 - accuracy: 0.6639 - val_loss: 0.7535 - val_accuracy: 0.6419\n",
      "Epoch 44/500\n",
      "74/74 [==============================] - 3s 39ms/step - loss: 0.7488 - accuracy: 0.6605 - val_loss: 0.7511 - val_accuracy: 0.6419\n",
      "Epoch 45/500\n",
      "74/74 [==============================] - 3s 46ms/step - loss: 0.7221 - accuracy: 0.6588 - val_loss: 0.7395 - val_accuracy: 0.6554\n",
      "Epoch 46/500\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.7290 - accuracy: 0.6453 - val_loss: 0.7322 - val_accuracy: 0.6554\n",
      "Epoch 47/500\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.7293 - accuracy: 0.6622 - val_loss: 0.7399 - val_accuracy: 0.6284\n",
      "Epoch 48/500\n",
      "74/74 [==============================] - 3s 41ms/step - loss: 0.7317 - accuracy: 0.6605 - val_loss: 0.7300 - val_accuracy: 0.6351\n",
      "Epoch 49/500\n",
      "74/74 [==============================] - 3s 46ms/step - loss: 0.7164 - accuracy: 0.6639 - val_loss: 0.7186 - val_accuracy: 0.6622\n",
      "Epoch 50/500\n",
      "74/74 [==============================] - 3s 41ms/step - loss: 0.7041 - accuracy: 0.7010 - val_loss: 0.7123 - val_accuracy: 0.6757\n",
      "Epoch 51/500\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.6857 - accuracy: 0.6824 - val_loss: 0.7071 - val_accuracy: 0.6757\n",
      "Epoch 52/500\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 0.7048 - accuracy: 0.6706 - val_loss: 0.6937 - val_accuracy: 0.6892\n",
      "Epoch 53/500\n",
      "74/74 [==============================] - 4s 50ms/step - loss: 0.6733 - accuracy: 0.6858 - val_loss: 0.6932 - val_accuracy: 0.6757\n",
      "Epoch 54/500\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.6674 - accuracy: 0.6909 - val_loss: 0.6883 - val_accuracy: 0.6757\n",
      "Epoch 55/500\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.6985 - accuracy: 0.6807 - val_loss: 0.6910 - val_accuracy: 0.6824\n",
      "Epoch 56/500\n",
      "74/74 [==============================] - 3s 38ms/step - loss: 0.6857 - accuracy: 0.6757 - val_loss: 0.6673 - val_accuracy: 0.7230\n",
      "Epoch 57/500\n",
      "74/74 [==============================] - 2s 34ms/step - loss: 0.6625 - accuracy: 0.6740 - val_loss: 0.6636 - val_accuracy: 0.7230\n",
      "Epoch 58/500\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 0.6712 - accuracy: 0.7010 - val_loss: 0.6625 - val_accuracy: 0.7027\n",
      "Epoch 59/500\n",
      "74/74 [==============================] - 3s 47ms/step - loss: 0.6535 - accuracy: 0.7111 - val_loss: 0.6558 - val_accuracy: 0.7162\n",
      "Epoch 60/500\n",
      "74/74 [==============================] - 3s 40ms/step - loss: 0.6695 - accuracy: 0.6875 - val_loss: 0.6316 - val_accuracy: 0.7162\n",
      "Epoch 61/500\n",
      "74/74 [==============================] - 3s 35ms/step - loss: 0.6610 - accuracy: 0.7145 - val_loss: 0.6166 - val_accuracy: 0.7432\n",
      "Epoch 62/500\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.6864 - accuracy: 0.6892 - val_loss: 0.6241 - val_accuracy: 0.7568\n",
      "Epoch 63/500\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.6527 - accuracy: 0.7162 - val_loss: 0.6199 - val_accuracy: 0.7568\n",
      "Epoch 64/500\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.6488 - accuracy: 0.7044 - val_loss: 0.5926 - val_accuracy: 0.7905\n",
      "Epoch 65/500\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.6365 - accuracy: 0.7078 - val_loss: 0.5901 - val_accuracy: 0.7770\n",
      "Epoch 66/500\n",
      "74/74 [==============================] - 3s 36ms/step - loss: 0.6575 - accuracy: 0.7230 - val_loss: 0.5906 - val_accuracy: 0.7770\n",
      "Epoch 67/500\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 0.6171 - accuracy: 0.7399 - val_loss: 0.5883 - val_accuracy: 0.7635\n",
      "Epoch 68/500\n",
      "74/74 [==============================] - 3s 39ms/step - loss: 0.6310 - accuracy: 0.7162 - val_loss: 0.5896 - val_accuracy: 0.7703\n",
      "Epoch 69/500\n",
      "74/74 [==============================] - 3s 39ms/step - loss: 0.6644 - accuracy: 0.6959 - val_loss: 0.5938 - val_accuracy: 0.7770\n",
      "Epoch 70/500\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.6427 - accuracy: 0.6841 - val_loss: 0.5936 - val_accuracy: 0.7635\n",
      "Epoch 71/500\n",
      "74/74 [==============================] - 3s 39ms/step - loss: 0.6591 - accuracy: 0.7061 - val_loss: 0.5907 - val_accuracy: 0.7703\n",
      "Epoch 72/500\n",
      "74/74 [==============================] - 3s 46ms/step - loss: 0.6005 - accuracy: 0.7551 - val_loss: 0.5788 - val_accuracy: 0.7838\n",
      "Epoch 73/500\n",
      " 5/74 [=>............................] - ETA: 4s - loss: 0.6985 - accuracy: 0.6000"
     ]
    }
   ],
   "source": [
    "# Deep Neural Networks:\n",
    "import tensorflow as tf; print('We\\'re using TF-{}.'.format(tf.__version__))\n",
    "# import keras; print('We\\'re using Keras-{}.'.format(keras.__version__))\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, Flatten, BatchNormalization,\n",
    "                                     Conv1D, Conv2D, MaxPooling1D, MaxPooling2D,\n",
    "                                     LSTM, GRU, Embedding, Bidirectional, Concatenate)\n",
    "from tensorflow.keras.regularizers import (l1, l2, l1_l2)\n",
    "from tensorflow.keras.optimizers import (RMSprop, Adam, SGD)\n",
    "from tensorflow.keras.models import (Sequential, Model)\n",
    "\n",
    "# Core:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Performance:\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, matthews_corrcoef, precision_score, roc_curve, auc)\n",
    "from sklearn.model_selection import (StratifiedKFold, KFold, train_test_split)\n",
    "\n",
    "#Utilities:\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical as labelEncoding   # Usages: Y = labelEncoding(Y, dtype=int)\n",
    "from tensorflow.keras.utils import plot_model                        # Usages: plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=False, expand_nested=True)\n",
    "\n",
    "#end-import\n",
    "\n",
    "def lossPlot(results):\n",
    "    plt.title(label='Loss: Training and Validation')\n",
    "    plt.plot(results.history['loss'], label='Training Loss')\n",
    "    plt.plot(results.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#end-def\n",
    "\n",
    "def accuracyPlot(results):\n",
    "    plt.title(label='Accuracy: Training and Validation')\n",
    "    plt.plot(results.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(results.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#end-def\n",
    "\n",
    "def rocPlot(TPR, meanFPR):\n",
    "    plt.plot([0,1], [0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "    meanTPR = np.mean(TPR, axis=0)\n",
    "    meanAUC = auc(meanFPR, meanTPR)\n",
    "    plt.plot(meanFPR, meanTPR, color='blue',\n",
    "            label=r'Mean ROC (AUC = %0.2f )' % (meanAUC),lw=2, alpha=1)\n",
    "\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Receiver Operating Characteristic Curve (ROC Curve)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('ROC-740.png')\n",
    "    plt.show()\n",
    "#end-def\n",
    "\n",
    "T = 15 # terminus_length\n",
    "\n",
    "X1 = np.load('bpf-740.npy')\n",
    "X2 = np.load('bits-740.npy')\n",
    "X3 = np.load('blosum-740.npy')\n",
    "\n",
    "\n",
    "X1 = X1[:,0:T,:]\n",
    "X2 = X2[:,0:T,:]\n",
    "X3 = X3[:,0:T,:]\n",
    "\n",
    "\n",
    "Y  = [1 for _ in range(376)]\n",
    "Y += [0 for _ in range(364)]\n",
    "\n",
    "Y = labelEncoding(Y, dtype=int)\n",
    "\n",
    "\n",
    "print(X1.shape)\n",
    "print(X2.shape)\n",
    "print(X3.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "### Model-740\n",
    "\n",
    "def Network():\n",
    "    ### Head-1:\n",
    "    input1 = Input(shape=X1[0].shape)\n",
    "\n",
    "    x = Conv1D(filters=10, kernel_size=4, padding='same', activation='relu', kernel_regularizer=l2(l=0.01))(input1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.80)(x)\n",
    "\n",
    "    x = Conv1D(filters=8, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(l=0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.70)(x)\n",
    "\n",
    "    head1 = Flatten()(x)\n",
    "\n",
    "\n",
    "    ### Head-2:\n",
    "    # input2 = Input(shape=X2[0].shape)\n",
    "\n",
    "    # x = Conv1D(filters=10, kernel_size=4, padding='same', activation='relu', kernel_regularizer=l2(l=0.01))(input2)\n",
    "    # x = BatchNormalization()(x)\n",
    "    # x = Dropout(rate=0.70)(x)\n",
    "\n",
    "    # x = Conv1D(filters=8, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(l=0.01))(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    # x = Dropout(rate=0.70)(x)\n",
    "\n",
    "    # head2 = Flatten()(x)\n",
    "\n",
    "\n",
    "    ### Head-3:\n",
    "    input3 = Input(shape=X3[0].shape)\n",
    "\n",
    "    x = Conv1D(filters=10, kernel_size=4, padding='same', activation='relu',)(input3)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.70)(x)\n",
    "\n",
    "    x = Conv1D(filters=8, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(l=0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.70)(x)\n",
    "\n",
    "    head3 = Flatten()(x)\n",
    "\n",
    "\n",
    "    # merge\n",
    "    merge = Concatenate()([head1, head3])\n",
    "\n",
    "    output = Dense(units=8, activation='relu', kernel_regularizer=l2(l=0.01))(merge)\n",
    "    output = BatchNormalization()(output)\n",
    "    output = Dropout(rate=0.70)(output)\n",
    "\n",
    "    output = Dense(units=2, activation='softmax')(output)\n",
    "\n",
    "    return Model(inputs=[input1, input3], outputs=[output])\n",
    "#end-def\n",
    "\n",
    "model = Network()\n",
    "model.summary()\n",
    "plot_model(model, to_file='model-740.png', show_shapes=True, show_layer_names=False, expand_nested=True)\n",
    "\n",
    "setEpochNumber     = 500     # Performed-welled in epoch 600.\n",
    "setBatchSizeNumber = 8\n",
    "####################################################\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=101)\n",
    "\n",
    "Accuracy = []\n",
    "Sensitivity = []\n",
    "Specificity = []\n",
    "Precision = []\n",
    "MCC = []\n",
    "\n",
    "# ROC Curve:\n",
    "fig1 = plt.figure(figsize=[12,12])\n",
    "\n",
    "TPR = []\n",
    "meanFPR = np.linspace(0, 1, 100)\n",
    "\n",
    "i = 1\n",
    "\n",
    "# CM = np.array([\n",
    "#      [0, 0],\n",
    "#      [0, 0],\n",
    "# ], dtype=int)\n",
    "\n",
    "for train, test in cv.split(Y):\n",
    "\n",
    "    # Compile Model:\n",
    "    model = Network()\n",
    "    model.compile(optimizer=Adam(lr=0.005),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Run Model:\n",
    "    results = model.fit(x=[X1[train,:,:], X3[train,:,:]],\n",
    "                        y=Y[train,:],\n",
    "                        validation_data=([X1[test,:,:], X3[test,:,:],], Y[test,:]),\n",
    "                        batch_size=setBatchSizeNumber, epochs=setEpochNumber,\n",
    "                        verbose=1,\n",
    "                        callbacks=[])\n",
    "\n",
    "    # Evaluate the Model:\n",
    "    accuracy = model.evaluate(x=[X1[test,:,:], X3[test,:,:]], y=Y[test,:])\n",
    "    Accuracy.append(accuracy[1])\n",
    "\n",
    "    # Performance Metices:\n",
    "    Yactual = Y[test,:].argmax(axis=1)\n",
    "    Yp = model.predict([X1[test,:,:], X3[test,:,:]])\n",
    "    v = Yp\n",
    "    Yp = Yp.argmax(axis=1)\n",
    "\n",
    "    CM = confusion_matrix(y_pred=Yp, y_true=Yactual)\n",
    "    TN, FP, FN, TP = CM.ravel()\n",
    "\n",
    "    MCC.append(matthews_corrcoef(y_true=Yactual, y_pred=Yp))\n",
    "    Sensitivity.append( TP / (TP + FN) )\n",
    "    Specificity.append( TN / (TN + FP) )\n",
    "    Precision.append(precision_score(y_true=Yactual, y_pred=Yp))\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(Yactual, v[:,1])\n",
    "    TPR.append(interp(meanFPR, fpr, tpr))\n",
    "    rocauc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, rocauc))\n",
    "    i= i+1\n",
    "\n",
    "    # # Performance Plot\n",
    "    # print('#################################################')\n",
    "    # print('Fold\\'s Accuracy: {:.2f}'.format(accuracy[1]*100.0))\n",
    "    # lossPlot(results)\n",
    "    # accuracyPlot(results)\n",
    "    # print('#################################################')\n",
    "\n",
    "#end-for\n",
    "\n",
    "rocPlot(TPR, meanFPR)\n",
    "\n",
    "print(Accuracy)\n",
    "print('Accuracy: {:.2f}'.format(np.sum(Accuracy)/5.0))\n",
    "print('Sensitivity: {0:.4f}'.format(np.sum(Sensitivity)/5.00))\n",
    "print('Specificity: {0:.4f}'.format(np.sum(Specificity)/5.00))\n",
    "print('MCC: {0:.4f}'.format(np.sum(MCC)/5.00))\n",
    "print('Precision: {0:.4f}'.format(np.sum(Precision)/5.00))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
